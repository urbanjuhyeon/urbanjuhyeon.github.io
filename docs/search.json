[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Juhyeon Park",
    "section": "",
    "text": "I am an urban planner and data scientist working to improve the use of urban data as the evidence base for formulating urban policies, plans, strategies, and programs.\nMy particular areas of interest are:\n\nUrban analytics\nData mining/visualization\nSmart cities\n\nI use this site to showcase my recent work, and discuss new data-science and statistical tools and techniques on my blog.\n\n\nUlsan National Institute of Science and Technology PhD in Urban and Environmental Engineering | 2022\nUlsan National Institute of Science and Technology B.S. in Urban and Environmental Engineering | 2015\n\n\n\nPostdoctoral Researcher at Ulsan National Institute of Science and Technology | February 2022 - present\n\n\n\nDownload my resumé."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "한국어 감성 분석 비교: 네이버 vs 구글\n\n\n\n\n\n\n\nR\n\n\n\n\n대표적인 한국어 감성 분석 API를 제공하는 구글과 네이버 성능 비교\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\n주거상담기록 데이터 분석 (6): 주제 공변량 분석\n\n\n\n\n\n\n\nR\n\n\n\n\n토픽모델링 주제와 메타데이터 간 분석\n\n\n\n\n\n\nMar 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\n주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보\n\n\n\n\n\n\n\nPython\n\n\n\n\n토픽모델링 주제의 비중과 분포 정보 추출\n\n\n\n\n\n\nMar 25, 2023\n\n\n\n\n\n\n  \n\n\n\n\n주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명\n\n\n\n\n\n\n\nR\n\n\n\n\n토픽모델링의 주제 이름을 주요 단어, 대표문서를 참고하여 명명\n\n\n\n\n\n\nMar 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\n주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정\n\n\n\n\n\n\n\nR\n\n\n\n\n토픽모델링의 최종 주제 개수 결정\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\n주거상담기록 데이터 분석 (2): 형태소분석\n\n\n\n\n\n\n\nPython\n\n\n\n\n텍스트 정제와 형태소분석을 포함한 텍스트 전처리 과정\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\n주거상담기록 데이터 분석 (1): 개요\n\n\n\n\n\n\n\n주거상담기록\n\n\n텍스트마이닝\n\n\n\n\n주거상담기록 텍스트 데이터를 분석하는 시리즈의 개요\n\n\n\n\n\n\nMar 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR를 활용하여 Open API로 공개된 데이터 수집하기\n\n\n\n\n\n\n\nR\n\n\nOpen API\n\n\n\n\nOpen API로 공개된 공공데이터를 R로 수집해보자\n\n\n\n\n\n\nFeb 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR를 활용하여 구글 지도 데이터 수집하기\n\n\n\n\n\n\n\nR\n\n\nGoogle Maps\n\n\n\n\n구글 지도에 등록된 POI 정보를 R을 활용하여 모아보자\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\n창업 맛보기: 2022년 한국형 아이코어 프로그램 참가 후기\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nJuhyeon Park\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juhyeon Park",
    "section": "",
    "text": "I am an urban planner and data scientist. I am currently a postdoctoral researcher at the Urban Planning & Analytics Lab at UNIST. My interests include urban analytics, Data mining/visualization, and smart cities.\nWith this blog, I intend to make a good quality of the blogs that have inspired me during my urban research.   I hope it helps to provide valuable insights and resources for fellow enthusiasts of the field."
  },
  {
    "objectID": "posts/2022-08-21-what-i-learn-startup-program/index.html",
    "href": "posts/2022-08-21-what-i-learn-startup-program/index.html",
    "title": "창업 맛보기: 2022년 한국형 아이코어 프로그램 참가 후기",
    "section": "",
    "text": "이 글은 2022년 한국형 아이코어 사업에 참가한 여정을 돌아보고 느낀점을 정리한 포스트다. 겪었던 경험과 생각을 정리하는 것에서 나아가 이후 사업 참여를 계획하는 사람들이 참고한다면 좋을 것 같다. 창업을 생각하는 대학원생에게 아주 좋은 프로그램이라고 생각하지만 구체적인 사업 과정이나 후기가 없어서 아쉬웠다."
  },
  {
    "objectID": "posts/2022-08-21-what-i-learn-startup-program/index.html#사업-소개",
    "href": "posts/2022-08-21-what-i-learn-startup-program/index.html#사업-소개",
    "title": "창업 맛보기: 2022년 한국형 아이코어 프로그램 참가 후기",
    "section": "사업 소개",
    "text": "사업 소개\n한국형 아이코어는 Lab-to-Market형 기술창업 교육 사업이다. 연구실 기술 기반의 기초·원천 결과물이 학문적인 연구성과에서 그치지 않고 시장에 활용될 수 있는지를 빠르게 검증하는 것이 본 사업의 목적이다. 과학기술정보통신부의 창업교육사업으로서 2015년부터 매년 진행되어 왔으며, 개인적으로는 대학생(원)이 창업 맛보기를 할 수 있는 가장 좋은 프로그램이라고 생각한다. 사업에 대한 개요와 구성은 여기를 참고하면 된다.\n\n\n무엇을 얻어갈 수 있는가?\n\n1. 고객 중심 마인드\n대학원에서 훈련한 개발자 중심 마인드에서 시장 고객 중심의 비즈니스 마인드를 배울 수 있다. 연구실에서 기술 개발을 하다보면 기술이 실현가능한지(feasibility)에 시간을 많이 쓰곤 한다. 이런 기술은 스타트업에서 무언가를 만들기 위해서 필요하겠지만 스타트업을 성공시키는 것은 아니다.\n기술 수준을 높이기 보다도 그 기술 기반 제품이 어떤 고객에게 필요로 하며, 호감을 가지고 갖고 싶어하는지desirable를 고민하는데 스타트업은 시간을 써야한다. 이 프로그램에서 이 고민을 체계적으로 수행하는 방법에 대해 배울 수 있다.\n\n\n2. 창업가 마인드\n\n\n3. 오픈 마인드"
  },
  {
    "objectID": "posts/2022-08-21-what-i-learn-startup-program/index.html#지원-및-선정",
    "href": "posts/2022-08-21-what-i-learn-startup-program/index.html#지원-및-선정",
    "title": "창업 맛보기: 2022년 한국형 아이코어 프로그램 참가 후기",
    "section": "지원 및 선정",
    "text": "지원 및 선정\n\n사업 지원 동기\n3월 학교에서 사업시행공고를 담은 홍보메일 보고 사업을 처음 알게 됐다. 당시 2월 학위를 받으며 익힌 WiFi 센싱 기술이나 연구실 과제로 배운 텍스트마이닝 기술 등으로 논문이나 학회 발표, 특허가 아닌 더 실용적인 성과물을 만들고 싶었다. 도시계획 분야에서 학술연구로 주로 써본 기술들을 시장에 적용하면 어떨까라는 생각으로 지원하게 되었다.\n\n\n신청서 작성\n사업에 지원하기 위해선 신청서를 작성해야 한다. 어려운 부분 없이 작성 요령도 참고할 수 있어 아이템만 정해진다면 신청서 작성은 크게 어렵지 않다.\n나는 센싱 기술과 텍스트마이닝 기술, 스마트 기술로 공공공간의 질을 측정하는 패키지를 지자체에 파는 아이템을 구상했다. 지금 보면 학술연구와 거의 다를 것이 없는데, 신청서 작성 당시 시간이 부족해 구상한 다른 아이디어를 구체화하기 어려웠다. 스마트 기술은 그대로 가되, 이것을 지자체가 아닌 일반인(소상공인)에게 팔고 싶다는 생각이 자주 들었지만 이 방향으로 쓰진 못했다.\n작성시 개념도를 만들면 좋다. 신청서에 아이템 설명을 적을 때도 유용하지만 선정 이후 아이템을 소개할 상황이 많기 때문이다. 나는 아래 개념도를 위주로 신청서를 작성했다. \n\n\n발표 평가\n서류 평가를 통과했다는 메일을 한달 후에 받고 4월 18일 발표 평가를 했다. 평가는 비대면(온라인)으로 제출기한까지 1) 발표자료(15p 이내 PDF), 2) 발표영상(7분이내 발표자료를 설명하는 Zoom 녹화본)을 만들어 보냈다.\n발표 당일은 정해진 시간에 Zoom 링크를 통해 평가장에 입장하고 사전에 보낸 7분 발표영상을 심사위원 4~5명과 같이 본 후, 질의응답 시간을 7분 가졌다. 주로 제시한 기술 설명과 한계점을 물어봤다. 어떤 부분이 독창적인지, 제시한 특허와 어떻게 관련되었는지를 대답했다. 해외교육을 신청했기에 미국에서 가능한 사업인지에 대답하고, 영어로 질문하고 답하는 시간도 마지막에 짧게 가졌다.\n발표 후, 일주일 내로 최종 선정 메일을 받았다. 아이템에 도움 되는 참고 의견도 같이 보내준다.\n\n\n\n\n\nAPI-intro"
  },
  {
    "objectID": "posts/2022-08-21-what-i-learn-startup-program/index.html#기초-창업교육",
    "href": "posts/2022-08-21-what-i-learn-startup-program/index.html#기초-창업교육",
    "title": "창업 맛보기: 2022년 한국형 아이코어 프로그램 참가 후기",
    "section": "기초 창업교육",
    "text": "기초 창업교육\n이론 위주로 고객 탐색하는 방법론에 대해 주로 배운다. 사업 시작을 알리는 부트캠프와 3일간 합숙교육, 기초교육으로 구성된다.\n\n부트캠프\n6월 8일, 선정된 탐색팀이 처음으로 모여 아이코어 프로그램에 대한 소개를 듣고 탐색팀 간 네트워크를 하는 OT 시간을 가졌다.\n1부는 전반적인 사업 소개와 혁신단별 인스트럭터 등 사업 관계자 인사, 그리고 이전 기수의 사례 발표가 진행된다. 2부는 각 혁신단별로 모여서 1분 내외로 자신의 아이템을 설명하고 만들어준 명함을 돌리며 서로 알아가는 시간을 가진다. \n\n\n기초교육\n창업 아이템으로 비즈니스 모델을 만들어보고 이를 검증하는 고객탐색방법론에 대해 배운다. 2박 3일간 합숙교육과 온라인 중간 점검, 최종 발표로 구성되어 있으며, 공식 일정은 여기를 참고하면 된다.\n\n합숙교육 1일차\n6월 25일, 1일차 오전에는 강당에 모여 한국형 아이코어 교육 과정의 개요와 목표에 대해 배운다. 교육 과정은 고객 가치를 무엇보다도 강조한다. - 스타트업 실패 요인이 기술과 자금 부족, 창업 시기 문제가 아닌 고객이 없기 때문 - 검증된 고객 가치를 담은 비즈니스 모델을 찾을 때까지 실패하며 수정과 보완(pivoting)하는 것이 필요 - 이 실패 과정은 빨라야 하며(fail fast), 성공한 비즈니스 모델을 찾은 다음, 마케팅 비용을 투입하고 기업을 설립하여 비즈니스를 확장하는 것이 적절 - 위 과정을 모델화한 것이 스티브블랭크의 고객 개발 모델 (참고)\n다음엔 BMC(Business Model Canvas), 자신의 스타트업 아이템의 비즈니스 모델을 구체화하는 과정을 돕는 그래픽 템플릿을 배운다. 사업에 필요한 9개 핵심 요소로 구성된 BMC를 만들다보면 내가 어떤 사업을 하려는지를 체계적으로 고민할 수 있다 (참고).\n\n\n\n기초교육\n창업 아이템으로 비즈니스 모델을 만들어보고 이를 검증하는 고객탐색방법론에 대해 배운다. 2박 3일 꽉찬 교육으로 공식 일정은 여기를 참고하면 된다.\n\n기초교육 1일차\n6월 25일, 1일차 오전에는 강당에 모여 강의를 들었다. 먼저 한국형 아이코어 교육 과정의 개요와 목표를 배웠고 주요 내용은 다음과 같다. - 고객 가치를 중심으로 생각하라 - 스타트업 실패 요인은 기술과 자금 부족, 창업시기 문제가 아닌 고객이 없기 때문 - 스타트업은 실패의 연속이며, 고객 가치가 검증된 비즈니스 모델을 찾을 때까지 실패하면서 수정·보완(pivoting) 필요 - 주요 가설을 고객 인터뷰를 통해 검증하고 빠른 실패(fail fast)를 겪는 것이 필요 - 성공가능한 비즈니스 모델을 찾은 다음, 마케팅 비용을 투입하고 기업을 설립하여 비즈니스 확장\n위 과정을 모델화한 것이 스티브블랭크의 고객 개발 모델이다. 더 자세한 정보는 여기에서 볼 수 있다.\n다음 교육에서는 BMC(Business Model Canvas)를 배웠다. - BMC란? 스타트업 아이템의 비즈니스 모델을 구체화하는 과정을 돕는 그래픽 탬플릿 - 사업에 필요한 9개 핵심 요소로 구성된 BMC를 만들다보면 내가 어떤 사업을 하려는지를 체계적으로 고민 (여기 참고) >>>>>>> Stashed changes\n오후에는 혁신단별로 방에 따로 들어가 앉아 자신의 아이템으로 BMC를 작성하고 발표하는 시간을 가졌다. 1시간 동안 BMC를 BCB 웹사이트에 작성하고 과제 1을 만들어 업로드 한다. 이후 각 팀별로 과제 1을 5분 동안 발표하고 5분 피드백을 듣는다. 저녁을 먹고나서는 인스트럭터와 1:1로 10분간 피드백 시간을 별도로 가진다.\n\n\n합숙교육 2일차\n2일차는 고객 인터뷰를 수행하는 방법과 모의 인터뷰 실습을 진행한다. 오전에는 고객 인터뷰 필요성과 설계, 수행 방법에 대해 배운다. 인터뷰가 귀찮은 과정은 맞지만 하지 않았을 때 겪는 어려움은 이 두 클립을 보면 알 수 있다 (1, 2).\n오후에는 배운 인터뷰 내용을 실습하는 시간을 가진다. 점심 이후 1시간 동안 인터뷰 수행을 위한 비즈니스 가설 및 인터뷰 전략을 구상한다. 이후 3시간 동안 혁신단 다른 팀과 다른 혁신단 팀들과 모의 인터뷰를 진행한다.  총 10팀의 모의 인터뷰 수행건을 정리한 과제 2는 2일차 밤까지 업로드한다. 1일차와 마찬가지로 저녁 이후 인스트럭터와 피드백 시간을 가지며 인터뷰 수행한 것에 대한 조언을 듣는다.\n\n\n합숙교육 3일차\n3일차는 교육 없이 합숙교육으로 배운 내용을 오전에 발표했다. BMC로 사업 컨셉을 설명하고, 모의 인터뷰로 검증하고자 하는 것을 가치제안모델과 비즈니스모델 가설 페이지로 전달했다. 이후 각 인터뷰를 정리한 고객인터뷰기록과 최종 인사이트 도출 페이지로 모의 인터뷰 수행 결과를 발표했다.\n\n\n\n\n\n여기\n\n\n\n\n최종발표\n합숙 교육이 끝나고 최종발표까지 2주간 실제 고객 인터뷰 15건을 수행한다. \n이 인터뷰를 정리한 과제 3으로 10분 발표와 5분 피드백 시간을 가진다. 앞서 말한대로 인터뷰로 검증되지 않은 비즈니스 모델인 경우 수정 및 보완 하는 것이 당연하므로, 이 과제 3도 이 과정이 포함된다.  \n피봇 과정을 거친다면 BMC도 바뀌게 된다. 처음 BMC와 바뀐 BMC도 발표하며, 얼마나 바뀐 지를 인스트럭터 평가자가 눈여겨 본다. \n\n\n기초교육 소결\n\n1. 준비한 아이템으로 기초교육은 끝까지 수행하고 최종 발표에서 수정하자\n우리 팀은 기초교육 첫 단추를 잘못 끼워 교육 내내 고생했다. 과제 1 발표부터 우리는 제출한 아이템의 고객인 공공공간 관리자, 공무원이 아닌 소상공인으로 첫페이지부터 소개를 했었다. 교육(인터뷰)를 수행하지도 않고 아이템을 바꾸는 경우는 지원서 합격을 위해 그럴싸한 아이템으로 지원하고 바꾸는 경우로 의심받을 수 있다고 한다. 인터뷰 이후 수정 및 보완은 아주 당연한 과정이므로, 처음 제출한 아이템이 이상하더라도 계속해서 교육을 수행하고 마지막에 바꾸기를 추천한다.\n\n\n2. 준비한 아이템으로 BMC 미리 만들어오자\n1시간만에 BMC를 완성하기가 정말 쉽지 않아서 미리 생각해오는게 필요하다. 아래는 만든 첫번째 BMC인데 잘못된 점이 아주 많지만 기록으로 여기 올려둔다. \n\n\n3. 모의 인터뷰는 상대에게 구체적인 페르소나를 부여하자\n모의 인터뷰를 하면서 느낀건 인터뷰 상대는 우리와 같은 스타트업 창업자지 진정한 고객이 아니므로, 페르소나를 구체적으로 부여해야 한든 것이다. 고객을 구체적으로 설정하면 좋은 것과 같지, 모의 인터뷰 이전 상대 팀에게 우리 아이템에 대해 밝히지 않으면서 되었으면 하는 고객에 대해 최대한 구체적으로 말하는게 좋았다. 우리는 처음에는 일반적인 소상공인에서, 인터뷰를 수행하면서 카페 사장님, 개인 카페 사장님, 프리미엄 카페 운영하는 사장님으로 좁혀나갔고 더 유의미한 결과를 얻었다."
  },
  {
    "objectID": "posts/2022-08-21-what-i-learn-startup-program/index.html#실전교육",
    "href": "posts/2022-08-21-what-i-learn-startup-program/index.html#실전교육",
    "title": "창업 맛보기: 2022년 한국형 아이코어 프로그램 참가 후기",
    "section": "실전교육",
    "text": "실전교육\n실전 교육은 국내형과 해외형으로 나눠진다. 우리 팀은 해외형 실전교육을 수행했고 미국 서부로 가 3주간 교육을 받았다. 각 1주를 Core week으로 부르고 수요일마다 모여서 프로그램 경과를 발표한다. 마지막 날은 Demo day로 최종 발표를 하고 교육은 마무리 된다. 교육 과정은 여기에서 볼 수 있다.\n\n실전교육 1주차\n미국 도착 다음날 Core week 1 발표가 바로 있다. 프로그램을 등록하고 소개를 간략히 받은 다음, 팀별로 발표가 시작되고 팀 소개를 먼저 한다. 우리는 음식점 온라인 리뷰를 분석해서 고객 경험을 이해하고 다중 평가 기준 점수를 제공해서 1) 방문객에게는 맛집 추천을 2) 소상공인 음식점 주인에게는 컨설팅 서비스를 제공하려고 했다.  이후 인터뷰 수행 결과와 계획에 대해서 발표한다. 미국에서 인터뷰 기회가 현실적으로 없지만 인터뷰 결과를 발표해야 하므로, 최종 발표 이후 국내 인터뷰를 꾸준히 더 수행해야 한다.  ### 실전교육 2주차 1주차 수요일부터 일주일간 인터뷰 수행 결과를 2주차 수요일에 발표한다. 첫 부분은 팀 소개와 BMC이고 수정된 것을 발표해도 괜찮다. 우리 팀은 의사결정에 도움을 주는 시스템(decision support system)으로 이름을 바꾸고, 30건이 추가되어 총 35건 인터뷰를 수행했으며 BMC에서는 1) 방문객들에게 맛집 검색 시간을 1/5로 줄여줄 것을, 2) 소상공인에게는 온라인 리뷰 기반 리포트를 제공할 것을 구분지어 발표했다.  이후 인터뷰 수행 리포트는 우리가 생각했던 것(가설)과 수행 했던 것(실험), 배운 것(결과), 향후 계획(토론)을 담아 만든다. 인터뷰를 수행하기 전 이 부분에 채워야할 것을 생각하고 설계한다면 더 도움이 될 것 같다.  발표 마지막엔 BMC를 보여준다. 업데이트된 고객과 고객 문제, 가치 제안을 포함해서 1주차에서 배운 Channels과 Revenue Streams을 추가한다. \n\n\n실전교육 3주차\n\n\n\n\n\n데모데이\n공식적으로 마지막 날인 데모데이는 다른 교육 없이 프로그램을 성공적으로 수행한 것을 축하하는 날이다. 10여개 팀이 선정되어 10분 내외로 프로그램 수행한 것을 발표하고 인스트럭터와 외부 평가의원이 5분간 의견을 준다. \n\n\n해외 인터뷰 팁\n해외 실전교육은 정량평가 기준인 75개 인터뷰 개수를 채웠는 지가 중요하다. 수월한 인터뷰 수행을 위한 팁은 다음과 같다.\n\n1. 출국 전 인터뷰를 잡아두자\n우리 팀은 미국 출국 전부터 인터뷰를 잡기 위해 노력했다. 영어가 출중하다면 관계 없지만 보다 수월한 인터뷰 수행을 위해, 그리고 미국에 사는 한국인 이야기도 들어봐야 한다고 생각했다.\n다른 방법보다도 링크드인으로 동문들에게 연락하는 방법이 가장 유효했다. UNIST에서 학위를 받은 사람 중 샌프란시스코에서 포스닥을 하거나 직장을 잡으신 분에게 일촌 신청과 메세지를 보냈다. 간략한 내 소개와 메세지를 보내는 이유인 인터뷰 요청, 그리고 간략한 아이템 소개를 담았다.  대부분이 긍정적인 반응과 함께 인터뷰에 응해주셨다. 샌프란시스코 인근 대학인 UCB와 스탠포드에서 연구하는 포스닥 분들을 만날 수 있었고 감사하게도 앞서 말한 시간보다 더 길게 인터뷰를 진행하며 아이템에 대한 고민을 나눌 수 있었다.  #### 2. 인터뷰 전략을 세우자 ##### 일반인 대상 많은 팀이 일반인을 대상으로 고객 탐색 인터뷰를 진행한다. 미국 일반인들도 인터뷰를 구하는 우리의 첫인상을 중요하게 생각할 것이다. 우리는 셔츠나 구두 등 격식을 차리는 복장에 나눠준 명찰을 매고 매 인터뷰에 임했다. 처음 말을 걸 땐, 오늘 하루 어때? 날씨 좋지? 같은 미국식으로 접근하는 방법도 있지만 나의 경우, 정석적으로 명찰을 보여주며, UCB에서 지원하는 창업프로그램에 참여하는 학생인데 고객 탐색 인터뷰를 위해 5분 시간을 내줄 수 있는 지를 묻는 것이 더 반응이 좋았다. \n\n소상공인 대상\n우리 팀의 다른 고객은 음식점을 운영하는 소상공인이었다. 무턱대고 미국인이 운영하는 음식점에 찾아가 인터뷰를 구하기 힘들다고 판단한 우리는 한인 음식점을 집중적으로 공략했다. 먼저 샌프란시스코 다운타운과 인근 지역 한식 음식점을 목록화하고 최대한 밥을 그곳에서 사먹고 나오면서 명찰을 드리며 인터뷰를 요청했다. 특히, 한 사장님과 좋은 관계를 유지하면 다른 분들도 소개해주시는 경우도 있었다. \n\n\n\n3. 설문조사가 아닌 대화를\n처음 인터뷰는 써놓은 대본을 읽고 예/아니오를 체크하고 넘어가는 경우가 많았다. 상대방 눈을 바라보지 못하고 대화가 아닌 물어보고 답을 적고 하는 것은 인터뷰가 아니라고 한다.  인터뷰에서 물어볼 내용이 많아도 한 질문에 대해 관심이 있다면 그 근본 원인을 찾기 위해 5-Why 기법으로 다른 질문을 과감히 버리고 집중했다. 녹음 기능을 켜고 최대한 눈을 마주보고 대화를 하려고 했고, 이 경우에 더 많은 시사점을 얻을 수 있었다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "",
    "text": "구글 지도에는 도시 분석에 활용할 수 있는 다양한 정보가 등록되어 있다. 아래는 서울 연남동을 구글 지도에 검색한 결과다.\n\n이 범위 내, 음식점이 얼마나 많이 있을까? 평점이 높은 음식점은 어디에 몰려있을까? 구글 지도에 등록된 음식점, 카페와 같은 POI 정보를 수집하면 이에 답할 수 있다. 구글 API와 R을 활용하여 이 구글 지도 POI를 수집하는 방법에 대해 아래 다루고자 한다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#클라우드-및-결제-설정",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#클라우드-및-결제-설정",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "1.1. 클라우드 및 결제 설정",
    "text": "1.1. 클라우드 및 결제 설정\n\n구글 클라우드에서 콘솔을 생성한다. \n계정 정보와 본인 확인, 연락처 정보 및 결과 정보를 차례로 입력한다. \n이후, 구글 클라우드 플랫폼에서 결제 설정을 한다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#api-생성-및-설정",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#api-생성-및-설정",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "1.2. API 생성 및 설정",
    "text": "1.2. API 생성 및 설정\n\n사용할 API를 생성한다. \n사용할 서비스를 활성화시켜준다. \nPlaces API 외 googleway R 패키지를 활용하기 위해서는 Geocoding API 등 자신이 원하는 기능을 설정해주어야 한다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#api-키-복사",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#api-키-복사",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "1.3. API 키 복사",
    "text": "1.3. API 키 복사\n\nR에서 사용할 구글 API key는 아래에서 복사해서 사용한다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#googleway",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#googleway",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "2.1. Googleway",
    "text": "2.1. Googleway\nGoogleway은 R를 활용하여 Google API 요청과 응답 등을 도와주는 패키지다. 구글 지도 외 구글 스트리트뷰 이미지, 자전거 루트 검색 등 다양한 구글 API를 활용한 기능을 제공하며, 이 글에서는 Google Places API를 주로 활용한다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#google-places-api",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#google-places-api",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "2.2. Google Places API",
    "text": "2.2. Google Places API\nGoogle Places API는 전세계 구글 지도에 등록된 POI 정보를 수집하는 데 쓰인다. 이 POI 정보는 우리가 구글 지도에서 검색하는 것과 같으며, 구글은 검색 방식을 3가지로 구분한다.\n\n\n\n\n\n\n\n\n검색 방법\n설명\n검색 예시\n\n\n\n\nText\n텍스트 검색\n마포구 공원\n\n\nNearby\n특정 장소(위도/경도) 주변 검색\n(37.557, 126.924) 주변 500m 중식집\n\n\nPlace Detail\n특정 POI에 관한 구체적인 정보\n연남동 소이연남 별점, 리뷰 수 등\n\n\n\n각 방법에 대해 Googleway로 요청하고 응답 받는 방법은 아래와 같다.\n\nText\ntext 검색은 원하는 키워드를 구글 지도에서 검색하는 것이다. 마포구 공원을 검색하는 것을 예시로 들어보자.\n\nlibrary(googleway)\n\nres <- google_places(\n  search_string = \"마포구 공원\", # 검색하는 키워드\n  key = key, # 자신의 구글 API key\n  language = \"ko\", # 한국어 설정\n  )\n\n보낸 쿼리 응답에서 장소 이름과 평균 평점을 뽑아내면 다음과 같다.\n\n> cbind(res$results$name, res$results$rating)\n      [,1]                  [,2] \n [1,] \"경의선숲길공원\"      \"4.6\"\n [2,] \"마포 어린이 공원\"    \"3.9\"\n [3,] \"서울함 공원\"         \"4.2\"\n [4,] \"노을공원\"            \"4.6\"\n [5,] \"하늘공원\"            \"4.5\"\n [6,] \"망원한강공원\"        \"4.4\"\n [7,] \"경의선숲길\"          \"4.4\"\n [8,] \"성산근린공원\"        \"4.2\"\n [9,] \"평화의공원\"          \"4.5\"\n[10,] \"월드컵공원\"          \"4.5\"\n[11,] \"경의선공원길\"        \"4.6\"\n[12,] \"한강공원 마포나들목\" \"4.3\"\n[13,] \"경의선숲길공원\"      \"4.5\"\n[14,] \"근린공원\"            \"4\"  \n[15,] \"조각공원\"            \"3.8\"\n[16,] \"와우산체육공원\"      \"4.2\"\n[17,] \"복사꽃어린이공원\"    \"3.9\"\n[18,] \"한강공원망원지구\"    \"4.2\"\n[19,] \"합정공원\"            \"3.7\"\n[20,] \"윗잔다리공원\"        \"4\"  \n\n이외에도 user_ratings_total (구글 리뷰 총 개수), formatted_address (주소), geometry.location.lat/lng (위치좌표) 등이 수집된다.\n추가로, 쿼리 응답은 상위 20개만 받을 수 있다. 그 외 데이터를 수집하기 위해서는 next_page_token을 활용하면 된다.\n\nres_next <- google_places(\n  search_string = \"마포구 공원\", # 검색하는 키워드\n  key = key, # 자신의 구글 API key\n  language = \"ko\", # 한국어 설정\n  page_token = res$next_page_token # 기존 쿼리의 next_page_token\n  )\n\n\n\nNearby\nnearby 검색은 특정 좌표 기반 검색이며, POI 유형과 주변 검색 범위 등을 설정할 수 있다. 홍대입구역(위도: 126.924, 경도: 37.557) 근처 50m 내 위치한 음식점을 예시로 들어보자.\n\n> res <- google_places(\n+   location = c(37.557, 126.924), # 홍대입구역 위치 좌표\n+   place_type = \"restaurant\", # POI type\n+   radius = 50, # 주변 N meter\n+   language = \"ko\", # 한국어\n+   key = key # 자신의 구글 API key\n+   ) \n> \n> cbind(res$results$name, res$results$rating, res$results$place_id)\n     [,1]                       [,2]  [,3]                         \n[1,] \"스시메이진 홍대점\"        \"3.5\" \"ChIJR64R9cKYfDURFdUwoHMV9QI\"\n[2,] \"PASTA e PIZZA\"            \"4\"   \"ChIJt6EGjcKYfDURFFHDTbzzr7M\"\n[3,] \"애슐리\"                   \"4.5\" \"ChIJD5kR8MKYfDURrgorWW5NDqw\"\n[4,] \"서가엔쿡 홍대입구점\"      \"4\"   \"ChIJnYYHQcGZfDURU6FbPQ2VQGU\"\n[5,] \"아비꼬 홍대입구역 EXIT점\" \"3\"   \"ChIJG-v8wraZfDURKfxX4RBsfIk\"\n\n\n\nPlace Detail\nPlace Detail 검색은 특정 POI에 대한 리뷰 정보를 수집하기 위해 활용된다. 구글에서 지정한 POI id인, place_id 기반이며, 위 예시에서 스시메이진 홍대점을 예시로 들어보자.\n\n> det <- google_place_details(\n+   place_id = \"ChIJR64R9cKYfDURFdUwoHMV9QI\", # 스시메이진 홍대점 id\n+   key = key, \n+   language = \"ko\"\n+   )\n> \n> cbind(det$result$reviews$author_name, substr(det$result$reviews$text, 1, 20))\n     [,1]              [,2]                                  \n[1,] \"김진억\"          \"평일 점심 가격대 가성비 좋다.\\n다만\" \n[2,] \"youngsun hong\"   \"개인적으로 다시 가진 않을 것 같습니\" \n[3,] \"최석하\"          \"처음 가봤는데 음식도 맛있어서 좋았습\"\n[4,] \"케이린_게임세상\" \"홍대역 바로 앞이라 그런지 가격은 좀\" \n[5,] \"불꽃의초롱이\"    \"홍대역 8번쪽에 있는 스시메이진 홍대\" \n\n\n\n\n\n\n\nImportant\n\n\n\n각 POI별 리뷰는 최대 5개까지만 응답받을 수 있다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#r-실습-전",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#r-실습-전",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "3.1. R 실습 전",
    "text": "3.1. R 실습 전\n실습 질문은 아래와 같다.\n\"서울 연남동 내 카페은 몇 개고, 리뷰 개수와 평점이 가장 높은 곳은 어디인가?\"\n간단하게 접근하면 google_places를 사용해서 연남동 중심 좌표를 기준으로 모든 cafe POI를 수집하는 쿼리를 보내고, 그 쿼리의 next_page_token를 계속해서 넣으며 수집하면 된다.\n하지만, 구글 API에서는 한 쿼리당 60개 이상 아이템을 수집하지 못한다. (link)\n해결 방법은 다음과 같다.\n\n연남동 지역을 m (meter) 간격으로 grid 나누기\n한 grid 중심에서 n (meter) 이내 cafe POI를 수집 \n\n\n\n\n\n\n\nImportant\n\n\n\n적절한 m과 n을 설정해야 한다. 아주 작은 값은 시간이 오래걸리고 구글 API 청구 비용이 증가하며, 너무 크다면 한 grid 내 POI가 60개가 넘어가 제대로 수집되지 않을 것이다."
  },
  {
    "objectID": "posts/2023-01-15-how-to-use-googleway-r/index.html#r-실습",
    "href": "posts/2023-01-15-how-to-use-googleway-r/index.html#r-실습",
    "title": "R를 활용하여 구글 지도 데이터 수집하기",
    "section": "3.2. R 실습",
    "text": "3.2. R 실습\n\n패키지 불러오기\n필요한 패키지를 불러오고 없다면 설치한다.\n\n# Package reading\nPackages <- c(\"data.table\", \"stringr\", \"ggplot2\", \"sf\", \"tidyr\", \"sp\", \"googleway\", \"ggmap\", \"dplyr\")\ninstall_pkgs <- function(pkgs) {\n  # 신규 패키지 설치 \n  new_pkgs <- pkgs[!(pkgs %in% installed.packages()[, \"Package\"])]\n  if (length(new_pkgs))\n    install.packages(new_pkgs, dependencies = TRUE)\n  \n  # 기존 패키지 library 불러오기\n  sapply(pkgs, require, character.only = TRUE)\n}\n\ninstall_pkgs(Packages)\nlapply(Packages, require, character.only = TRUE)\n\n## Projection\nproj_WGS84 <- CRS(\"+init=epsg:4326\") \nproj_TM <- CRS(\"+init=epsg:5186\")\n\n\n\nGrid 만들기\n본 실습은 grid 간격(m)은 100m, 검색 범위(n)는 66.67으로 설정했으며, 이유는 아래와 같다.\n\n구글 지도에서 연남동을 검색하고 카페 POI를 살펴보았을 때, 100m X 100m grid 내 60개 이상 카페 POI가 존재하지 않아보인다. 따라서 100m를 가로와 세로 변으로 하는 grid를 만든다.\n100m X 100m grid 중심점에서 검색하는 범위는 변의 반지름(50m)보다 조금 더 크면 좋을 것이다. 따라서 1/3을 더해서 66.67m (100 X (1/2) X (1 + 1/3)) 내 POI를 검색하게끔 한다.\n\n위 작업을 그림으로 나타내면 다음과 같다. \n아래는 R로 위 방식을 나타내는 것이다. 연남동 shapefile은 여기에서 다운받을 수 있다.\n\nvalue_gridInterval <- 100 # Grid 간격\nvalue_radiusSearch <-\n  value_gridInterval * (2/3) # 실제 검색 범위, 검색 단위에서 1/3 더한 값\n\nshp_bdry <- st_read(\"data/boundary_yeonnam.shp\")\n\nshp_bdry_grid <- shp_bdry %>%\n  st_make_grid(., value_gridInterval)\n\n\n\nGrid별 수집\n만든 grid는 구글 API에 활용되기 전 다음 과정을 거치고 각 grid별로 POI 정보를 수집한다.\n\nshp_bdry_grid_split <- shp_bdry_grid %>% \n  st_transform(proj_WGS84) %>% # 좌표계 변환\n  st_centroid() %>% # Grid 중심점\n  st_coordinates %>% as.data.table %>% # 중심점 좌표 정보\n  group_split(id_grid = row_number()) # 각 grid별로 list\n\ndb_1a <- rbindlist(lapply(shp_bdry_grid_split, function(pnt){\n  \n  df_places_final <- NULL\n  \n  print(pnt$id_grid)\n  \n  df_places <- googleway::google_places(\n    location = c(pnt$Y, pnt$X),\n    place_type = \"cafe\",\n    radius = value_radiusSearch,\n    language = \"ko\",\n    key = key) \n  \n  if(length(df_places$results) != 0){\n    \n    df_places_results <- df_places$results\n    geometry <- df_places_results$geometry$location\n    df_places_results <- df_places_results %>% \n      select(one_of(c(\n        \"name\", \"place_id\", \"types\", \n        \"user_ratings_total\", \"rating\", \"vicinity\", \"business_status\")))\n    df_places_results <- cbind(df_places_results, geometry)\n    \n    \n    while (!is.null(df_places$next_page_token)) {\n      print(df_places$next_page_token)\n      print(df_places$status)\n      Sys.sleep(5) # time to not overload the Google API\n      \n      df_places <- googleway::google_places(\n        location = c(pnt$Y, pnt$X),\n        place_type = \"cafe\",\n        radius = value_radiusSearch,\n        language = \"ko\",\n        page_token = df_places$next_page_token,\n        key = key) \n      \n      df_places_next <- df_places$results\n      \n      if (length(df_places_next) > 0){\n        geometry <- df_places_next$geometry$location\n        df_places_next <- df_places_next %>%\n          select(one_of(c(\n            \"name\", \"place_id\", \"types\",\n            \"user_ratings_total\", \"rating\", \"vicinity\", \"business_status\")))\n        df_places_next <- cbind(df_places_next, geometry)\n        df_places_results <- bind_rows(\n          df_places_results, df_places_next)\n      }\n      Sys.sleep(2) # time to not overload the Google API\n    }\n    \n    # df_places_final <- bind_rows(\n    #   df_places_final, df_places_results)\n    \n    df_places_final <- cbind(df_places_results, pnt)\n  } \n\n}), fill = TRUE)\n\n\n\n데이터 전처리\n수집한 데이터는 다음과 같다. 811개 카페 POI가 수집되었고, types 중 cafe가 포함된 구글 POI가 모두 수집된 것이다.\n\ndb_1a\n\n               name                    place_id\n  1:           모뎐 ChIJn7PjZ0mZfDURDGztfRvBelc\n  2:     베이글카페 ChIJV2uwPNyYfDURdLuGbg9wkCA\n  3:     커피사랑방 ChIJ6cZZMtyYfDURIPO2h_PNYzk\n  4:       카페엘리 ChIJNWPNA92YfDUR6_7knu3MWEI\n  5: (주)요거프레소 ChIJsWLtVNyYfDURZ_f71SWvPZI\n ---                                           \n807:     연희단팥죽 ChIJAUntXomffDURf4LLUJ6iQy0\n808:         부어크 ChIJc4LMR_KYfDURgwcNhxlAT7g\n809:      카페12911 ChIJaXtlR_KYfDURs4vcvYDRuIg\n810:       어굿이어 ChIJ_6t1KheZfDURH4dte1WIvgc\n811:       컬러드빈 ChIJYQt8pV6ZfDURd7CKUTlNfi8\n                                               types user_ratings_total rating\n  1:       cafe|food|point_of_interest|establishment                 10    4.5\n  2:       cafe|food|point_of_interest|establishment                  2    4.0\n  3:       cafe|food|point_of_interest|establishment                  7    4.6\n  4:       cafe|food|point_of_interest|establishment                  4    4.0\n  5:       cafe|food|point_of_interest|establishment                 NA     NA\n ---                                                                          \n807:       cafe|food|point_of_interest|establishment                 64    4.4\n808:       cafe|food|point_of_interest|establishment                  5    3.8\n809:       cafe|food|point_of_interest|establishment                  1    3.0\n810: cafe|food|point_of_interest|store|establishment                 NA     NA\n811:       cafe|food|point_of_interest|establishment                 20    4.6\n                                                vicinity business_status\n  1:                       마포구 서교동 동교로19길 52-7     OPERATIONAL\n  2:               마포구 서교동 449-21번지 동공빌라 1층     OPERATIONAL\n  3: 서교동 247-20번지 임오빌딩 1층 마포구 서울특별시 KR     OPERATIONAL\n  4:        동교동 203-10번지 102호 마포구 서울특별시 KR     OPERATIONAL\n  5:          연남동 571-10번지 4층 마포구 서울특별시 KR     OPERATIONAL\n ---                                                                    \n807:                           서대문구 연희로11가길 8-5     OPERATIONAL\n808:                               서대문구 연희동 126-9     OPERATIONAL\n809:        연희동 129-11번지 1층 서대문구 서울특별시 KR     OPERATIONAL\n810:                     서대문구 연희로11나길 7-5 지1층     OPERATIONAL\n811:                    서대문구 연희동 연희로11가길 8-8     OPERATIONAL\n          lat      lng        X        Y id_grid\n  1: 37.55785 126.9173 126.9170 37.55798       1\n  2: 37.55763 126.9172 126.9170 37.55798       1\n  3: 37.55835 126.9173 126.9170 37.55798       1\n  4: 37.55783 126.9195 126.9193 37.55798       3\n  5: 37.55780 126.9189 126.9193 37.55798       3\n ---                                            \n807: 37.56789 126.9289 126.9283 37.56790     132\n808: 37.56756 126.9287 126.9283 37.56790     132\n809: 37.56747 126.9286 126.9283 37.56790     132\n810: 37.56733 126.9283 126.9283 37.56790     132\n811: 37.56809 126.9290 126.9283 37.56790     132\n\n\n이 데이터는 전처리 과정이 필요하다.\n\n연남동을 grid로 나눌 때, 끝점을 사각형으로 인지하고 grid로 생성했으므로 연남동 내 POI만 추출\n간단하게 리뷰가 5개 이상 존재하며 영업중인 POI만 추출\n\n\ndb_1b <- db_1a %>%\n  # 영업 중이며 리뷰 5개 이상 추출\n  filter(business_status == \"OPERATIONAL\" & user_ratings_total >= 5) %>%\n  # 데이터를 sf 형태로 변환\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = proj_WGS84) %>%\n  # 연남동 지역 내 POI만 추출\n  st_intersection(st_transform(shp_bdry, proj_WGS84)) %>%\n  # 좌표 정보 생성\n  mutate(lng = st_coordinates(.)[,1], \n         lat = st_coordinates(.)[,2]) %>%\n  # 지리정보 삭제\n  st_drop_geometry() %>% as.data.table()\n\n\n\n데이터 활용\n질문으로 돌아가보자.\n\"서울 연남동 내 카페은 몇 개고, 리뷰 개수와 평점이 가장 높은 곳은 어디인가?\"\n전처리 이후 연남동 내 카페 개수는 다음과 같다.\n\nlength(db_1b$place_id) # 필터링 후 총 개수\n\n[1] 229\n\n\n리뷰 개수가 가장 높은 곳은 이곳이다.\n\ndb_1b[order(-user_ratings_total)][1,]\n\n                name                    place_id\n1: 테일러커피 연남점 ChIJB_1mvO6YfDUR4cCrgtcLX-s\n                                       types user_ratings_total rating\n1: cafe|food|point_of_interest|establishment                727    4.2\n              vicinity business_status        X        Y id_grid    site\n1: 마포구 성미산로 189     OPERATIONAL 126.9261 37.56339      75 Yeonnam\n        lng      lat\n1: 126.9264 37.56306\n\n\n리뷰 평점이 가장 높은 곳(중에서 가장 리뷰 개수가 많은)은 이곳이다.\n\ndb_1b[order(-rating, -user_ratings_total)][1,]\n\n       name                    place_id\n1: 익명다방 ChIJ-aw70JGZfDURfvLogcyD-DQ\n                                       types user_ratings_total rating\n1: cafe|food|point_of_interest|establishment                 20      5\n                  vicinity business_status        X        Y id_grid    site\n1: 마포구 연남동 연희로 31     OPERATIONAL 126.9272 37.56159      54 Yeonnam\n        lng      lat\n1: 126.9268 37.56166\n\n\n수집한 카페 위치 분포는 아래와 같다."
  },
  {
    "objectID": "posts/2023-02-11-how-to-use-open-api/index.html",
    "href": "posts/2023-02-11-how-to-use-open-api/index.html",
    "title": "R를 활용하여 Open API로 공개된 데이터 수집하기",
    "section": "",
    "text": "공공데이터 등을 API로 공개한 경우가 최근 많아졌다. 아래는 공공데이터포털에서 상권을 검색한 결과 중 오픈 API로 제공되는 데이터 목록이다. \n첫번째 데이터의 경우, 전국 점포 DB를 한번에 csv 파일로 다운 받을 수 없다. Open API 형태로 상권이나 건물 등의 단위로 Open API를 통해 수집해야 한다."
  },
  {
    "objectID": "posts/2023-02-11-how-to-use-open-api/index.html#step-1-open-api-활용-신청",
    "href": "posts/2023-02-11-how-to-use-open-api/index.html#step-1-open-api-활용-신청",
    "title": "R를 활용하여 Open API로 공개된 데이터 수집하기",
    "section": "Step 1: Open API 활용 신청",
    "text": "Step 1: Open API 활용 신청\nOpen API 예시는 공공데이터포털에서 제공하는 소상공인시장진흥공단_상가(상권)정보를 대상으로 한다. 이곳에서 먼저 해당 API 활용신청을 한다."
  },
  {
    "objectID": "posts/2023-02-11-how-to-use-open-api/index.html#step-2-요청하기",
    "href": "posts/2023-02-11-how-to-use-open-api/index.html#step-2-요청하기",
    "title": "R를 활용하여 Open API로 공개된 데이터 수집하기",
    "section": "Step 2: 요청하기",
    "text": "Step 2: 요청하기\n신청을 완료하면 마이페이지로 이동하고, 해당 데이터 API를 클릭하고 요청 예시 중 하나인 건물 단위 상가업소 조회를 클릭하고 미리보기를 누른다."
  },
  {
    "objectID": "posts/2023-02-11-how-to-use-open-api/index.html#step-3-요청에-따른-응답보기",
    "href": "posts/2023-02-11-how-to-use-open-api/index.html#step-3-요청에-따른-응답보기",
    "title": "R를 활용하여 Open API로 공개된 데이터 수집하기",
    "section": "Step 3: 요청에 따른 응답보기",
    "text": "Step 3: 요청에 따른 응답보기\n위의 요청에서 미리보기를 누르면 URL로 API 요청이 입력되어, URL에 따른 응답을 볼 수 있다.\n\n요청으로 만들어진 URL 주소는 세가지로 구분할 수 있다.\n\n서비스명(건물 단위 상가업소 조회)\n서비스 key\n요청 변수\n\n\n이를 자세히 설명하면,\n\nhttps://apis.data.go.kr/B553077/api/open/sdsc2/은 소상공인진흥공단 상가(상권)정보 API 주소\nstoreListInBuilding?은 건물 단위 상가업소 조회\nserviceKey=g6aqHarEo~은 부여받은 API 인증키\nkey=3017011200113530000022216&은 요청 변수 중 key(건물관리번호) 지정\nindsLclsCd=Q&은 대분류가 Q(음식)에 해당하는 것만 조회\nindsMclsCd=Q12&은 중분류가 Q12(커피점/카페)에 해당하는 것만 조회\nindsSclsCd=Q12A01&은 소분류가 Q12A01(커피전문점/카페/다방)에 해당하는 것만 조회\nnumOfRows=100&은 최대 100개 항목을 조회\npageNo=1&은 첫번째 페이지를 요청\ntype=xml은 조회를 xml 형식으로 응답"
  },
  {
    "objectID": "posts/2023-03-21-housing-consultation-analysis-1/index.html",
    "href": "posts/2023-03-21-housing-consultation-analysis-1/index.html",
    "title": "주거상담기록 데이터 분석 (1): 개요",
    "section": "",
    "text": "들어가며\n이 시리즈는 텍스트 분석, 특히 토픽모델링 분석 과정에 대해 다룹니다.\nSH 서울주택도시공사에서 연구 용역을 수행하며, 서울 주거 관련 상담사가 기록한 텍스트 데이터를 다룰 기회가 있었습니다. 쉽게 접하지 못하는 좋은 데이터를 가지고 저는 데이터 정제와 분류할 카테고리를 지정하지 않고 텍스트를 분류하는 토픽모델링을 수행했습니다.\n\n\n\n연구용역 개요\n\n\n\n\n시리즈 개요\n제가 한 작업은 아래 그림과 같으며, 총 6개 블로그 글로 재구성했습니다.\n\n\n\n블로그 시리즈 개요\n\n\n\n주거상담기록 데이터 분석 (2): 형태소분석 (링크)\n주거상담기록 데이터 분석 (3): 주제 수 결정 (링크)\n주거상담기록 데이터 분석 (4): 주제 명명 (링크)\n주거상담기록 데이터 분석 (5): 주제 분석 (링크)\n주거상담기록 데이터 분석 (6): 주제 공변량 분석 (링크)\n\n\n\n\n\n\n\nNote\n\n\n\n주거상담 원데이터를 전처리하는 과정은 개인정보 보호 문제나 분석 목적에 따라 달라지므로, 별도로 다루지 않았습니다."
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "",
    "text": "비정형 텍스트 데이터는 정제와 토큰화 등 전처리 과정을 통해 텍스트 분석에 용이한 형태로 바꿔주어야 한다.\n\n정제는 불필요한 텍스트를 제거하고 동의어 사전으로 텍스트를 일관되게 만들어 주는 과정이다. 위 예시에서는 ’서울금융 복지센터’를 ’서울금융복지센터’로 바꾸었다. 토큰화는 형태소 분석과 사용자, 불용어 사전 적용으로 분석에 필요한 형태소를 추출하는 과정이다. 띄어쓰기를 기준 토큰 형태로 바꾸었다."
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#패키지-설치",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#패키지-설치",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "1.1. 패키지 설치",
    "text": "1.1. 패키지 설치\n필요한 패키지를 설치한다. 아래 코드를 Visual Studio Code 등 python 편집기에 입력하고 실행하면 된다.\n\n!pip install pandas\n!pip install tqdm\n!pip install kiwipiepy"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#패키지-및-데이터-불러오기",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#패키지-및-데이터-불러오기",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "1.2. 패키지 및 데이터 불러오기",
    "text": "1.2. 패키지 및 데이터 불러오기\n설치 후, 필요한 패키지를 먼저 불러온다.\n\n## 필요한 모듈 불러오기\nimport json, random, re, os\nimport pandas as pd\nimport swifter # 병렬처리\nfrom tqdm import tqdm # 작업 프로세스 시각화\nfrom kiwipiepy import Kiwi # 형태소분석기 모듈\nfrom kiwipiepy.utils import Stopwords # 불용어사전 담겨있는 모듈 불러오기\n\n샘플 데이터를 불러온다. 여기에서 다운로드 할 수 있다. id_f는 각 데이터 고유 ID, cons_text는 상담기록 원문이다.\n\ndb_record = pd.read_csv(\"data/db_record_v1_sample.csv\") # 데이터 불러오기\n\n\n\n\n\n\n  \n    \n      id_f\n      cons_text\n    \n  \n  \n    \n      1\n      * 재개발 임대 아파트 거주중.\n* 임대료 7개월 1,381,980원 체납\n* 일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함.\n* 작년 20.10.30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데, 수급자 신청해 보려 했으나, 실업급여와 중복으로 받을 수 없다고 안내를 받았다 함. \n* 임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니, 이번에 빌려서 어떻게든 해결해 보려 한다고 하심.  \n* 일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데, 필요하신지 여쭤보았더니, 자신이 직접 알아보시겠다고 하심.\n    \n    \n      2\n      * 청년임차보증금 이자지원사업 추천서 관련 문의전화 주심(20대 추정)\n* 건강보험자격득실확인서 지역가입자이며 대학생으로 소득 없음. 국세청홈택스 소득금액증명원 조회결과 2019년 아르바이트 소득 조회됨. 취업준비생 유형으로 신청 가능한지 궁금\n· 취업준비생 유형으로 신청 가능\n* 서울주거포털에서 추천서 신청시 거주지 주소에 현주소 기입하는 것인지 궁금\n· 서울주거포털 추천서 신청시 현주소 기입\n* 서울시추천서 신청과 이사 할 전월세주택 물색 중에서 어느것을 먼저 해야하는지 궁금\n· 서울시추천서 유효기간 발급일로부터 3개월임을 안내"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#불필요한-토큰-제거",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#불필요한-토큰-제거",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "1.3. 불필요한 토큰 제거",
    "text": "1.3. 불필요한 토큰 제거\n기호나 태그 등 불필요한 토큰을 삭제한다. 숫자나 영어도 불필요하다고 여길 수 있지만 ‘1인가구’, ‘2룸’, ‘LH’, ‘SH’ 등 의미있는 토큰이 많아 삭제하지 않았다.\n아래 코드로 먼저 텍스트 내 영어 표현을 모두 소문자화 한 후, 한글과 숫자, 영어 외 나머지를 공백 처리했다.\n\ndb_record['cons_text_cleaned_1'] = list(map(lambda text: re.sub(r\"([^가-힣0-9a-z ])\", \" \", text), db_record['cons_text'].str.lower()))\n\n\n\n\n\n\n  \n    \n      cons_text\n      cons_text_cleaned_1\n    \n  \n  \n    \n      * 재개발 임대 아파트 거주중.\n* 임대료 7개월 1,381,980원 체납\n* 일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함.\n* 작년 20.10.30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데, 수급자 신청해 보려 했으나, 실업급여와 중복으로 받을 수 없다고 안내를 받았다 함. \n* 임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니, 이번에 빌려서 어떻게든 해결해 보려 한다고 하심.  \n* 일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데, 필요하신지 여쭤보았더니, 자신이 직접 알아보시겠다고 하심.\n        재개발 임대 아파트 거주중    임대료 7개월 1 381 980원 체납   일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함    작년 20 10 30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데  수급자 신청해 보려 했으나  실업급여와 중복으로 받을 수 없다고 안내를 받았다 함     임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니  이번에 빌려서 어떻게든 해결해 보려 한다고 하심      일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데  필요하신지 여쭤보았더니  자신이 직접 알아보시겠다고 하심 \n    \n    \n      * 청년임차보증금 이자지원사업 추천서 관련 문의전화 주심(20대 추정)\n* 건강보험자격득실확인서 지역가입자이며 대학생으로 소득 없음. 국세청홈택스 소득금액증명원 조회결과 2019년 아르바이트 소득 조회됨. 취업준비생 유형으로 신청 가능한지 궁금\n· 취업준비생 유형으로 신청 가능\n* 서울주거포털에서 추천서 신청시 거주지 주소에 현주소 기입하는 것인지 궁금\n· 서울주거포털 추천서 신청시 현주소 기입\n* 서울시추천서 신청과 이사 할 전월세주택 물색 중에서 어느것을 먼저 해야하는지 궁금\n· 서울시추천서 유효기간 발급일로부터 3개월임을 안내\n        청년임차보증금 이자지원사업 추천서 관련 문의전화 주심 20대 추정    건강보험자격득실확인서 지역가입자이며 대학생으로 소득 없음  국세청홈택스 소득금액증명원 조회결과 2019년 아르바이트 소득 조회됨  취업준비생 유형으로 신청 가능한지 궁금   취업준비생 유형으로 신청 가능   서울주거포털에서 추천서 신청시 거주지 주소에 현주소 기입하는 것인지 궁금   서울주거포털 추천서 신청시 현주소 기입   서울시추천서 신청과 이사 할 전월세주택 물색 중에서 어느것을 먼저 해야하는지 궁금   서울시추천서 유효기간 발급일로부터 3개월임을 안내"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#동의어-사전",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#동의어-사전",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "1.3. 동의어 사전",
    "text": "1.3. 동의어 사전\n같은 의미지만 다른 표현으로 쓰이는 단어를 통일하기 위해 동의어 사전을 적용한다. 사전에 구축한 사전은 여기에서 다운로드 가능하며, 바꿔줄 단어beforeWord, 바꿀 단어afterWord로 구성된다.\n예를 들어, 서울주거포털은 ‘서울 주거포털’과 ’서울주거포털’로, ’아산사회복지재단’은 ’현대아산재단’, ‘아산재단’ 등으로 상담사마다 같은 의미지만 다르게 쓰는 표현을 일치시켜준다.\n\n\n\n\n\n  \n    \n       \n      beforeWord\n      afterWord\n    \n  \n  \n    \n      0\n      재개발 임대\n      재개발임대\n    \n    \n      1\n      재개발임대주택\n      재개발임대\n    \n    \n      2\n      서울 주거포털\n      서울주거포털\n    \n    \n      3\n      현대아산재단 \n      아산사회복지재단\n    \n    \n      4\n      아산재단\n      아산사회복지재단\n    \n    \n      5\n      아산사회복지 재단\n      아산사회복지재단\n    \n    \n      6\n      아산복지재단\n      아산사회복지재단\n    \n  \n\n\n\n동의어 사전으로 상담 원문 내용을 바꾸는 코드는 아래와 같다. 함수 replace_word는 한 개의 상담 내용cons_text 열마다 동의어사전dict_synonyms에서 beforeWord와 일치하는 구간이 발견되면 afterWord로 바꾼다.\n\n## 구축한 동의어사전 불러오기\ndict_synonyms = pd.read_csv('data/dict/dict_synonyms.csv')\n\n## 동의어사전을 적용하는 함수 정의\ndef replace_word(text):\n    for i in range(len(dict_synonyms['beforeWord'])): \n        try:\n            if dict_synonyms['beforeWord'][i] in text: # beforeWord가 상담내용에서 발견된다면\n                text = text.replace(dict_synonyms['beforeWord'][i], dict_synonyms['afterWord'][i]) # 해당하는 단어의 afterWord로 바꾸어라\n        except Exception as e: # 에러가 있다면 멈추고 알려주어라\n            print(f\"Error 발생 / 에러명: {e}\")\n            print(dict_synonyms['afterWord'][i])\n            print(text)\n    return text\n\ndb_record['cons_text_cleaned_2'] = db_record['cons_text_cleaned_1'].swifter.set_npartitions(npartitions = 12).apply(lambda x: replace_word(x))\n\n\n\n\n\n\n\n\n\n  \n    \n      cons_text\n      cons_text_cleaned_2\n    \n  \n  \n    \n      * 재개발 임대 아파트 거주중.\n* 임대료 7개월 1,381,980원 체납\n* 일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함.\n* 작년 20.10.30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데, 수급자 신청해 보려 했으나, 실업급여와 중복으로 받을 수 없다고 안내를 받았다 함. \n* 임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니, 이번에 빌려서 어떻게든 해결해 보려 한다고 하심.  \n* 일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데, 필요하신지 여쭤보았더니, 자신이 직접 알아보시겠다고 하심.\n        재개발임대 아파트 거주중    임대료 7개월 1 381 980원 체납   일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함    작년 20 10 30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데  수급자 신청해 보려 했으나  실업급여와 중복으로 받을 수 없다고 안내를 받았다 함     임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니  이번에 빌려서 어떻게든 해결해 보려 한다고 하심      일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데  필요하신지 여쭤보았더니  자신이 직접 알아보시겠다고 하심 \n    \n    \n      * 청년임차보증금 이자지원사업 추천서 관련 문의전화 주심(20대 추정)\n* 건강보험자격득실확인서 지역가입자이며 대학생으로 소득 없음. 국세청홈택스 소득금액증명원 조회결과 2019년 아르바이트 소득 조회됨. 취업준비생 유형으로 신청 가능한지 궁금\n· 취업준비생 유형으로 신청 가능\n* 서울주거포털에서 추천서 신청시 거주지 주소에 현주소 기입하는 것인지 궁금\n· 서울주거포털 추천서 신청시 현주소 기입\n* 서울시추천서 신청과 이사 할 전월세주택 물색 중에서 어느것을 먼저 해야하는지 궁금\n· 서울시추천서 유효기간 발급일로부터 3개월임을 안내\n        청년임차보증금 이자지원 추천서 관련 문의전화 주심 20대 추정    건강보험자격득실확인서 지역가입자이며 대학생으로 소득 없음  국세청홈택스 소득금액증명원 조회결과 2019년 아르바이트 소득 조회됨  취업준비생 유형으로 신청 가능한지 궁금   취업준비생 유형으로 신청 가능   서울주거포털에서 추천서 신청시 거주지 주소에 현주소 기입하는 것인지 궁금   서울주거포털 추천서 신청시 현주소 기입   서울시추천서 신청과 이사 할 전월세주택물색 중에서 어느것을 먼저 해야하는지 궁금   서울시추천서 유효기간 발급일로부터 3개월임을 안내"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#형태소분석기-kiwi",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#형태소분석기-kiwi",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "2.1. 형태소분석기 Kiwi",
    "text": "2.1. 형태소분석기 Kiwi\nKiwi 형태소분석기를 활용하여 텍스트 토큰화를 진행한다. Kiwi는 빠른 속도와 준수한 정확도, 그리고 사용자사전 수정과 적용이 용이해 형태소분석기로 선택했다.\n\n기본 설정\nKiwi()에서 설정을 바꿔줄 수 있다. kiwi.tokenize는 텍스트 원문을 형태소분석하는 함수다. 기본 설정으로 예제를 토큰화해보자.\n\nkiwi = Kiwi() # Kiwi 기본값 설정\nkiwi.tokenize(\"서울주거포털에서 재개발임대 정보를 알아보셰요\")\n\n결과는 토큰 형태로 반환되며, 원형form과 형태소tag, 시작위치start, 길이len으로 구성된다.\n\n\n[Token(form='서울', tag='NNP', start=0, len=2), \n Token(form='주거', tag='NNG', start=2, len=2), \n Token(form='포털', tag='NNG', start=4, len=2), \n Token(form='에서', tag='JKB', start=6, len=2), \n Token(form='재', tag='XPN', start=9, len=1), \n Token(form='개발', tag='NNG', start=10, len=2), \n Token(form='임대', tag='NNG', start=12, len=2), \n Token(form='장보', tag='NNP', start=15, len=2), \n Token(form='를', tag='JKO', start=17, len=1), \n Token(form='알', tag='VV', start=19, len=1), \n Token(form='어', tag='EC', start=20, len=1), \n Token(form='보', tag='VX', start=21, len=1), \n Token(form='세', tag='EC', start=22, len=1), \n Token(form='요', tag='JX', start=23, len=1)]\n\n\n\n\nKiwi 추가 설정\nKiwi에서 활용 가능한 설정을 추가한다. 여기에서 설정 사항을 확인할 수 있다. 설정한 예제는 아래와 같다.\n\n\n형태소 분석기에 활용 가능한 모델; 처리 시간은 늘어나지만 정확한 모델링이 가능한 모델로 설정\n오타 교정\n사용자정의사전 적용; 별도로 만든 txt 파일 활용\n불용어사전 적용; Kiwi에서 제공하는 기본 사전 활용"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#사용자정의-사전",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#사용자정의-사전",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "2.2. 사용자정의 사전",
    "text": "2.2. 사용자정의 사전\n\n사용자정의사전 소개\n사용자정의 사전은 별도로 구축한 사용자정의 단어가 형태소분석기 모델이 아닌 지정한 형태소로 구분되게끔 하기 위해 사용한다. Kiwi 설정 중 형태소분석기나 불용어사전 등은 기본적으로 제공하는 것을 사용해도 괜찮지만, 이 사용자정의 사전은 분석 목적과 데이터 특성에 맞게 개발하고 업데이트 해줘야 한다.\n사용자사전을 적용하지 않고 예제인 ’서울주거포털에서 확인하실 수 있습니다’를 토큰화해보자.\n\nkiwi = Kiwi(model_type = 'sbg', typos = 'basic')\nkiwi.tokenize('서울주거포털에서 확인하실 수 있습니다', stopwords = Stopwords())\n\n결과는 다음과 같다. Kiwi 형태소분석기는 ‘서울주거포털’은 ’서울’, ‘주거’, ’포털’로 각각 명사로 분리하여 결과를 내놓았다. 추후 텍스트 분석의 단위는 이렇게 분리된 토큰인 점을 감안하면, ’서울주거포털’은 이렇게 분리가 되선 안된다.\n\n\n[Token(form='서울', tag='NNP', start=0, len=2), \n Token(form='주거', tag='NNG', start=2, len=2), \n Token(form='포털', tag='NNG', start=4, len=2), \n Token(form='확인', tag='NNG', start=9, len=2), \n Token(form='시', tag='EP', start=12, len=1), \n Token(form='있', tag='VA', start=16, len=1), \n Token(form='습니다', tag='EF', start=17, len=3)]\n\n\n여기에서 사용자정의 사전으로 ’서울주거포털’을 하나의 명사로 취급하는 명령어를 add_user_word로 추가해보자.\n\nkiwi = Kiwi(model_type = 'sbg', typos = 'basic')\nkiwi.add_user_word('서울주거포털', 'NNP', 0)\nkiwi.tokenize('서울주거포털에서 확인하실 수 있습니다', stopwords = Stopwords())\n\n’서울주거포털’은 분리되지 않고 하나의 고유명사로 구분되었다.\n\n\n[Token(form='서울주거포털', tag='NNP', start=0, len=6), \n Token(form='확인', tag='NNG', start=9, len=2), \n Token(form='시', tag='EP', start=12, len=1), \n Token(form='있', tag='VA', start=16, len=1), \n Token(form='습니다', tag='EF', start=17, len=3)]\n\n\n\n\n사용자정의사전 구축\n사용자정의 사전을 구축하는 방법은 다음과 같다.\n\n동의어 사전 활용하기\n동의어 사전에서 정의한 고유명사를 사용자정의사전 구축에 활용한다. 동의어사전을 불러오고, 치환한 단어의 고유값을 추출한 후, 각 명사를 고유명사NNG와 이 구문이 Kiwi 형태소분석기보다 더 높은 점수를 받게끔 9점을 부여한다.\n\ndict_synonyms = pd.read_csv('data/dict/dict_synonyms.csv') # 동의어사전 불러오기\nlist_userDefined_synonyms = (dict_synonyms['afterWord'].str.strip().unique() + \"\\tNNG\\t9\").tolist() # 사용자 정의 사전 포맷으로 변경\n\n위 코드 결과는 다음과 같다. 이 사전을 활용하면 ‘서울주거포털’은 ’서울’+‘주거’+‘포털’이 아니라 ’서울주거포털’로, ’일자리센터’는 ’일자리’+’센터’가 아니라 ’일자리센터’로 분석될 것이다.\n\n\n서울주거포털  NNG 9\n일자리센터   NNG 9\n고용복지센터  NNG 9\n상담센터    NNG 9\n\n\n\n\n명사 외 형태소 지정하기\n동사 등 형태소는 따로 텍스트 파일을 만들어준다. 예를 들어, ‘주심’은 Kiwi 형태소분석기에서는 심판의 의미로 명사로 구분되지만 주거상담 기록에서는 ’서류를 주시다’를 줄여서 쓰는 경우가 많았다. 이 경우에는 ’주심’을 ’주’+‘시’+’다’로 분석되게 지정해야 한다.\n여기에서 다운받은 파일을 먼저 불러온다.\n\ndict_add = pd.read_table(\"data/dict/dict_userDefined_madeByMe.txt\") # 추가 사전 불러오기\n\n이 목록을 사용자정의 사전 포맷으로 바꿔준다. 위와 마찬가지로 높은 점수, 우선순위를 가지도록 9점을 가지게 만들어준다.\n\nlist_userDefined_add = (pd.read_table(\"data/dict/dict_userDefined_madeByMe.txt\", header = None, sep = \"-\")[0] + \"\\t9\").tolist() # 사용자 정의 사전 포맷으로 변경\n\n아와 같이 명사 외 형태소는 지정된다.\n\n\n주심  주/VV + 시/EP + 다/EC\n하셨슴 하/VV + 었/EP + 음/EC  9\n연락키로    연락/NNG + 하/XSV + 기/ETN + 로/JKB  9\n미납됨 미납/NNG + 되/VV + 다/EC    9\n\n\n\n\n위 두 사전 결합\n위 두 사전을 결합하여 최종 사용자정의 사전을 만들어준다.\n\ndict_userDefined = pd.DataFrame(pd.DataFrame(list_userDefined_synonyms + list_userDefined_add)[0].unique()) # 두 list 결합\ndict_userDefined.to_csv('data/dict/dict_userDefined.txt', index = False, header = False) # 새로운 사용자 정의 사전 저장하기\n\n\n\n사용자정의사전 적용\n추가 설정을 한 형태소 분석의 결과는 다음과 같다. 여기에서 만든 사전은 다운받을 수 있다.\n\nkiwi = Kiwi(model_type = 'sbg', typos = 'basic') # Kiwi 추가 설정\nkiwi.load_user_dictionary('data/dict/dict_userDefined.txt') # 사용자사전 추가\nkiwi.tokenize(\"서울주거포털을 소개하며 종결키로 함\", stopwords = Stopwords())\n\n\n\n[Token(form='서울주거포털', tag='NNG', start=0, len=6), \n Token(form='소개', tag='NNG', start=8, len=2), \n Token(form='종결', tag='NNG', start=13, len=4), \n Token(form='ᆷ', tag='ETN', start=18, len=1)]"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#불용어-사전",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#불용어-사전",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "2.3. 불용어 사전",
    "text": "2.3. 불용어 사전\nkiwi 기본 불용어 사전에서 거르지 못한 불용어를 거르기 위해 사전을 만든다. ‘대상자’와 ’서비스’, ‘주거’ 등은 자주 등장하지만 분석에 유의미한 단어는 아니다. 지역명과 상담사명 등도 마찬가지로 제거하면 좋다. 구축한 사전은 여기에서 다운로드 할 수 있다.\n\n\n\n\n\n  \n    \n       \n      type\n      word\n      pos\n    \n  \n  \n    \n      0\n      일반\n      대상자\n      NNG\n    \n    \n      1\n      일반\n      서비스\n      NNG\n    \n    \n      2\n      일반\n      주거\n      NNG\n    \n    \n      3\n      일반\n      주택\n      NNG\n    \n    \n      4\n      일반\n      가구\n      NNG\n    \n    \n      5\n      지역명\n      잠실2동\n      NNG\n    \n    \n      6\n      지역명\n      잠실3동\n      NNG\n    \n    \n      7\n      상담사명\n      박상준\n      NNG\n    \n    \n      8\n      상담사명\n      이종현\n      NNG\n    \n  \n\n\n\n\n한글자 사전\n단어가 한글자지만 중요한 것을 모아둔 사전이다. 보통 한글자는 의미가 잘 없다. 하지만 ‘돈’은 경제적 상황을 서술할 때 사용하며, ’좁’은 ’좁다’, ‘좁은 집’으로, ’낡’은 ’낡다’ 등으로 활용된다. 이 같은 중요한 한글자는 불용어로 걸러지지 않게끔 한다. 형태소 분석한 토큰이 아주 짧은 경우는 보통 제거한다. 여기에서 다운로드 할 수 있다.\n\n\n\n\n\n  \n    \n       \n      word\n    \n  \n  \n    \n      0\n      돈\n    \n    \n      1\n      좁\n    \n    \n      2\n      낡\n    \n    \n      3\n      커\n    \n    \n      4\n      크\n    \n    \n      5\n      작"
  },
  {
    "objectID": "posts/2023-03-22-housing-consultation-analysis-2/index.html#kiwi-적용하기",
    "href": "posts/2023-03-22-housing-consultation-analysis-2/index.html#kiwi-적용하기",
    "title": "주거상담기록 데이터 분석 (2): 형태소분석",
    "section": "2.4. Kiwi 적용하기",
    "text": "2.4. Kiwi 적용하기\n앞에서 소개한 내용을 종합하여 만든 형태소 분석은 아래와 같다.\n\nKiwi 형태소분석\nKiwi 추가 설정과 사용자정의사전을 추가해서 예제 파일을 형태소분석 해준다.\n\nkiwi = Kiwi(model_type = 'sbg', typos = 'basic') # Kiwi 추가 설정\nkiwi.load_user_dictionary('data/dict/dict_userDefined.txt') # 사용자사전 추가\nmorph_analysis = lambda x: kiwi.tokenize(x, stopwords = Stopwords()) if type(x) is str else None # 형태소 분석\ndb_record['cons_text_posTagged_kiwi'] = db_record['cons_text_cleaned_2'].swifter.apply(morph_analysis)\n\n\n\n\n\n\n\n\n\n  \n    \n      cons_text\n      cons_text_posTagged_kiwi\n    \n  \n  \n    \n      * 재개발 임대 아파트 거주중.\n* 임대료 7개월 1,381,980원 체납\n* 일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함.\n* 작년 20.10.30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데, 수급자 신청해 보려 했으나, 실업급여와 중복으로 받을 수 없다고 안내를 받았다 함. \n* 임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니, 이번에 빌려서 어떻게든 해결해 보려 한다고 하심.  \n* 일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데, 필요하신지 여쭤보았더니, 자신이 직접 알아보시겠다고 하심.\n      [Token(form='재개발임대', tag='NNG', start=2, len=5), Token(form='아파트', tag='NNG', start=8, len=3), Token(form='거주', tag='NNG', start=12, len=3), Token(form='임대료', tag='NNG', start=19, len=3), Token(form='7', tag='SN', start=23, len=1), Token(form='개월', tag='NNB', start=24, len=2), Token(form='1', tag='SN', start=27, len=1), Token(form='381', tag='SN', start=29, len=3), Token(form='980', tag='SN', start=33, len=3), Token(form='체납', tag='NNG', start=38, len=2), Token(form='일자리', tag='NNG', start=43, len=3), Token(form='구하', tag='VV', start=48, len=2), Token(form='중', tag='NNG', start=52, len=1), Token(form='수입', tag='NNG', start=56, len=2), Token(form='경제', tag='NNG', start=63, len=2), Token(form='어렵', tag='VA-I', start=69, len=3), Token(form='상황', tag='NNG', start=73, len=2), Token(form='인하', tag='VV', start=78, len=2), Token(form='ᆷ', tag='ETM', start=79, len=1), Token(form='작년', tag='NNG', start=84, len=2), Token(form='20', tag='SN', start=87, len=2), Token(form='10', tag='SN', start=90, len=2), Token(form='30', tag='SN', start=93, len=2), Token(form='체납', tag='NNG', start=96, len=2), Token(form='상담', tag='NNG', start=99, len=2), Token(form='대상자', tag='NNG', start=104, len=3), Token(form='현재', tag='MAG', start=109, len=2), Token(form='실업급여', tag='NNG', start=112, len=4), Token(form='180', tag='SN', start=118, len=3), Token(form='나오', tag='VV', start=125, len=2), Token(form='데', tag='NNB', start=131, len=1), Token(form='수급자', tag='NNG', start=134, len=3), Token(form='신청', tag='NNG', start=138, len=2), Token(form='보', tag='VX', start=142, len=1), Token(form='려', tag='EC', start=143, len=1), Token(form='으나', tag='EC', start=146, len=2), Token(form='실업급여', tag='NNG', start=150, len=4), Token(form='중복', tag='NNG', start=156, len=2), Token(form='받', tag='VV-R', start=161, len=1), Token(form='안내', tag='NNG', start=170, len=2), Token(form='받', tag='VV-R', start=174, len=1), Token(form='함', tag='NNP', start=178, len=1), Token(form='임대료', tag='NNG', start=184, len=3), Token(form='체납', tag='NNG', start=188, len=2), Token(form='기간', tag='NNG', start=191, len=2), Token(form='길어지', tag='VV', start=195, len=3), Token(form='ᆯ수록', tag='EC', start=197, len=3), Token(form='나중', tag='NNG', start=201, len=2), Token(form='퇴거', tag='NNG', start=205, len=2), Token(form='명령', tag='NNG', start=208, len=2), Token(form='받', tag='VV-R', start=212, len=1), Token(form='으시', tag='EP', start=213, len=2), Token(form='있', tag='VA', start=219, len=1), Token(form='안내', tag='NNG', start=223, len=2), Token(form='드리', tag='VV', start=225, len=2), Token(form='더니', tag='EC', start=227, len=2), Token(form='이번', tag='NNG', start=231, len=2), Token(form='빌리', tag='VV', start=235, len=2), Token(form='어떻게', tag='MAG', start=239, len=3), Token(form='든', tag='JX', start=242, len=1), Token(form='해결', tag='NNG', start=244, len=2), Token(form='보', tag='VX', start=248, len=1), Token(form='려', tag='EC', start=249, len=1), Token(form='ᆫ다고', tag='EC', start=251, len=3), Token(form='시', tag='EP', start=255, len=2), Token(form='일자리', tag='NNG', start=263, len=3), Token(form='구하', tag='VV', start=268, len=2), Token(form='시', tag='EP', start=270, len=1), Token(form='ᆫ다고', tag='EC', start=270, len=3), Token(form='시', tag='EP', start=275, len=1), Token(form='임대주택', tag='NNG', start=278, len=4), Token(form='입주자', tag='NNG', start=283, len=3), Token(form='분', tag='NNB', start=287, len=1), Token(form='께', tag='JKB', start=289, len=1), Token(form='일자리', tag='NNG', start=291, len=3), Token(form='상담', tag='NNG', start=295, len=2), Token(form='연계', tag='NNG', start=298, len=2), Token(form='드리', tag='VX', start=302, len=2), Token(form='데', tag='NNB', start=308, len=1), Token(form='필요', tag='NNG', start=311, len=2), Token(form='시', tag='EP', start=314, len=1), Token(form='ᆫ지', tag='EC', start=314, len=2), Token(form='여쭈', tag='VV', start=317, len=2), Token(form='보', tag='VX', start=319, len=1), Token(form='더니', tag='EC', start=321, len=2), Token(form='자신', tag='NNG', start=325, len=2), Token(form='직접', tag='MAG', start=329, len=2), Token(form='알아보', tag='VV', start=332, len=3), Token(form='시', tag='EP', start=335, len=1), Token(form='시', tag='EP', start=340, len=2)]\n    \n  \n\n\n\n형태소분석 한 결과를 이어준다. ‘Token(form=’서울주거포털’, tag=‘NNG’, start=0, len=6)’는 그대로 활용할 수 없으므로, Token의 form들을 append로 이어준다. 이어줄 때, 주요품사만 골라서 붙여준다.\n\n## 형태소분석된 것들을 join\ndict_stopword = pd.read_csv('data/dict/dict_stopword.csv')\ndict_oneChar = pd.read_csv(\"data/dict/dict_oneChar.csv\")\n\n## 품사 정의하기\nlist_pos_all = (\"N\", \"V\", \"M\", \"J\", \"E\", \"X\") # 주요 품사 모두\nlist_pos_main = (\"N\", \"V\") # 명사, 동사, 형용사만\n\n## 불용어 제거하고 형태소 결합하기\ndef tokenToText(tokens):\n    textJoinedMain = []\n    textJoinedAll = [] \n    for token, pos, _, _ in tokens:        \n        if (token not in list(dict_stopword['word']) and ( len(token) > 1 or (len(token) == 1 and token in list(dict_oneChar['word']) ) )): \n            if pos.startswith('V'): token = token + '다' # 동사라면 뒤에 '다'를 붙여서 자연스럽게 만들기\n            if pos.startswith(list_pos_main) : textJoinedMain.append(token)\n            if pos.startswith(list_pos_all) : textJoinedAll.append(token)\n\n    out = pd.Series([' '.join(textJoinedAll), ' '.join(textJoinedMain)])\n    return out\n\ndb_record[['cons_text_posTagged_all', 'cons_text_posTagged_main']] = db_record['cons_text_posTagged_kiwi'].swifter.set_npartitions(npartitions = 6).apply(lambda x: tokenToText(x))\n\n\n\n\n\n\n\n\n\n  \n    \n      cons_text\n      cons_text_posTagged_main\n    \n  \n  \n    \n      * 재개발 임대 아파트 거주중.\n* 임대료 7개월 1,381,980원 체납\n* 일자리를 구하는 중으로 수입이 없어 경제적으로 어려운 상황으로 인함.\n* 작년 20.10.30 체납 상담했던 대상자로 현재 실업급여로 180만원이 나오고 있는데, 수급자 신청해 보려 했으나, 실업급여와 중복으로 받을 수 없다고 안내를 받았다 함. \n* 임대료 체납 기간이 길어질수록 나중에 퇴거 명령을 받으실 수도 있다고 안내드렸더니, 이번에 빌려서 어떻게든 해결해 보려 한다고 하심.  \n* 일자리를 구하신다고 하셔서 임대주택 입주자 분들께 일자리 상담 연계해 드리고 있는데, 필요하신지 여쭤보았더니, 자신이 직접 알아보시겠다고 하심.\n      재개발임대 아파트 거주 임대료 체납 일자리 구하다 수입 경제 어렵다 상황 인하다 작년 체납 상담 실업급여 나오다 수급자 신청 실업급여 중복 안내 임대료 체납 기간 길어지다 나중 퇴거 명령 안내 드리다 빌리다 해결 일자리 구하다 임대주택 입주자 일자리 상담 연계 드리다 필요 여쭈다 알아보다"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "",
    "text": "토픽모델링에서 연구자가 사전에 지정하는 주제 개수를 결정하는 과정에 대해 다룬다."
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#데이터-및-패키지-불러오기",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#데이터-및-패키지-불러오기",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "1.1. 데이터 및 패키지 불러오기",
    "text": "1.1. 데이터 및 패키지 불러오기\n여기에서 먼저 샘플 데이터를 다운로드한다. 100개 주거상담기록을 임의로 추출한 것이다.\n필요한 패키지를 불러오고 샘플 데이터를 준비한다.\n\n## 패키지 불러오기\npacman::p_load(\n  \"tidyverse\", \"tidytext\", \"data.table\", \"ggplot2\", \"stm\", \"RColorBrewer\",\n  \"future\", \"furrr\", \"knitr\", \"rmarkdown\")\n\ndb_record_1a <- fread(\n  \"data/db_record_v2.csv\", \n  select = c(\"id_f\", \"cons_text\", \"cons_text_posJoined_main\") # ID와 상담 원문, 상담 주요 품사 열만 불러오기\n  )"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#희소단어-제거",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#희소단어-제거",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "1.2. 희소단어 제거",
    "text": "1.2. 희소단어 제거\n\n희소단어 정의\n희소단어는 분석 데이터에서 잘 등장하지 않는 단어를 말한다. 이 단어를 포함하면 분석 행렬은 희소성이 높은, 예를 들어 문서 n X 단어 m 행렬에서 수많은 0이 차지하는 행렬이 된다. 이는 처리 시간이나 정확도 측면에서 효율적이지 않아 모델링 전 제거해주는 것이 좋다.\n\n\n단어 빈도 표\n단어 빈도 분포를 보며 희소단어를 제거해보자. unnest_tokens를 활용하여 토큰을 띄어쓰기 기준으로 나누어주고 원문을 토큰화/나누어주고, count로 그 단어들을 센 후, 누적 비율을 구한다.\n\ntable_wordFreq <- db_record_1a %>%\n  unnest_tokens(word, cons_text_posJoined_main, \n                token = stringr::str_split, pattern = \" \") %>% # 띄어쓰기 기준\n  count(word, sort = TRUE) %>% # 단어 개수 세기\n  mutate(prop = cumsum(n) / sum(n)) # 누적합 구하기\n\n’안내’가 83번으로 가장 많이 등장한 토큰이고, 전체 35%를 차지한다. 그 다음으론 ’신청’이 74건으로 2번째로 많이 등장했으며 ’안내’와 ’신청’은 전체 총 6.7%를 차지한다.\n\n\n\n\n  \n\n\n\n\n\n희소단어 기준\n누적 비율로 희소단어 기준을 설정한다. 누적 비율은 90분위나 95분위, 99분위를 주로 사용한다. 여기서는 90분위보다 빈도가 적게 발생한 단어를 확인해보자.\n\ntable_wordFreq[prop >= 0.900]\n\n       word n      prop\n  1:   보통 1 0.9000429\n  2:   보호 1 0.9004719\n  3:   복용 1 0.9009009\n  4:   복지 1 0.9013299\n  5: 봄맞이 1 0.9017589\n ---                   \n230:   활동 1 0.9982840\n231:   황토 1 0.9987130\n232:   회신 1 0.9991420\n233:   효율 1 0.9995710\n234:   후원 1 1.0000000\n\n\n적은 샘플로 빈도가 1개인 경우만 여기에 해당됐다. 따라서 빈도 1개 이하인 경우를 희소단어 기준으로 정한다.\n\n\n\n\n  \n\n\n\n\n\n희소단어 리스트\n희소단어를 빈도로 무턱대고 제거하면 희소하지만 중요한 단어도 제거하게 된다. 따라서 사용자정의 사전으로 희소단어에 포함되더라도 제거되지 않도록 해야한다. 여기에서 사용자정의 사전은 다운 가능하다.\nsetdiff 차집합으로 희소단어에 포함되고 사용자정의 사전에 포함 안되는 단어 리스트를 추출한다.\n\nlist_userDefined <- fread(\"data/dict/dict_userDefined.txt\")\n\nlist_wordRemoved <- setdiff( # 차집합\n  setDT(table_wordFreq)[n <= 1]$word, # 빈도 기준 희소단어\n  list_userDefined # 사용자정의 사전\n)\n\n\n\n\n\n\nx\n\n\n\n\n연기\n\n\n주거상황\n\n\n주거돌봄서비스\n\n\n입주자격\n\n\n자가\n\n\n\n\n\n\n\n희소단어 제거\n희소단어를 제거하는 코드는 아래와 같다.\n\ndb_record_f <- db_record_1a %>% \n  unnest_tokens(word, cons_text_posJoined_main, token = stringr::str_split, pattern = \" \") %>% # 띄어쓰기 기준\n  # 희소 단어 제외\n  filter(!(word %in% list_wordRemoved)) %>%\n  # 제외한 단어들을 다시 이어 붙이기\n  group_by(id_f)  %>%\n  summarize(cons_text_posJoined_main = toString(word)) %>% ungroup() %>%\n  mutate(cons_text_posJoined_main = str_remove_all(cons_text_posJoined_main, pattern = \"//,|,\")) %>%\n  filter(str_count(cons_text_posJoined_main, pattern = \" \") > 0)"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#문서-단어-행렬",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#문서-단어-행렬",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "2.1. 문서-단어 행렬",
    "text": "2.1. 문서-단어 행렬\nstm 패키지 textProcessor로 문서-단어 행렬을 만들어준다.\n\nprocessed <- as_tibble(db_record_f) %>%\n  textProcessor(\n    documents = .$cons_text_posJoined_main, metadata = .,\n    lowercase = FALSE, # 소문자 고려하지 않음\n    removepunctuation = FALSE, # 문장부호 제거 X\n    removenumbers = FALSE, # 숫자 제거 X\n    removestopwords = FALSE, # 불용어 적용 X\n    stem = FALSE, \n    wordLengths = c(1, Inf), # 1글자 이상\n    language = \"na\" \n  )\n\nout <- prepDocuments(\n  processed$documents,\n  processed$vocab,\n  processed$meta,\n  lower.thresh = 1,\n  upper.thresh = Inf)\n\nsummary(out)\n\ndocs <- out$documents; vocab <- out$vocab; meta <- out$meta"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#모델링",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#모델링",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "2.2. 모델링",
    "text": "2.2. 모델링\n\n주제 개수 입력\n2개~50개 미만 주제 개수 각각 모델링을 수행하기 위해, 주제 개수를 입력한다.\n\nK = seq(2, 50, 1) # 주제 개수\n\n\n\n병렬처리 활용한 모델링\n병렬처리로 다수 토픽모델링을 동시에 수행한다. 수행해야 하는 모델 개수가 많아 순차적으로 주제 개수가 많아서 1개 코어만 사용하면 시간이 오래걸리기 때문이다.\n\n\n\nhttps://dcgerard.github.io/advancedr/09_future.html\n\n\n먼저 사용할 코어 개수를 지정한다. multisession은 병렬처리를 workers는 사용할 멀티코어 개수다.\n\nplan(multisession, workers = 2)\n\nfuture_map로 병렬처리를 진행한다. 함수를 살펴보면 모델링 하기 전에는 ‘STM_start_주제개수.csv’ 파일을 만들고, 모델링 이후 모델 성능을 ’stm_fitted_주제개수.csv’로 저장한다.\n\nfuture_map(\n  K, function(x){\n    gc()\n    fwrite(\n    data.table(x = x),\n    paste0(\"result/\", \"stm_start\", \"_\", x, \".csv\"))\n    \n    stm_fitted <- searchK(\n    docs, vocab, x, \n    data = meta\n    )\n    \n    fwrite(stm_fitted$results, \n    paste0(\"result/\", \"stm_fiited_\", x, \".csv\")\n    )\n    \n    }\n  )"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#모델성능-결과-불러오기",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#모델성능-결과-불러오기",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "3.1. 모델성능 결과 불러오기",
    "text": "3.1. 모델성능 결과 불러오기\n모델 성능 결과가 모인 폴더 내에서 ’stm_fiited’이 들어간 파일을 읽고 그래프로 보일 수 있게끔 작업한다.\n\noutput_mergeSearchK <- rbindlist(\n  map(list.files(path = paste0(\"result\"), pattern = \"stm_fiited\", full.names = TRUE), fread)) %>%\n  pivot_longer(cols = c(\"semcoh\", \"heldout\", \"residual\", \"lbound\"),\n               names_to = \"index\", values_to = \"value\", values_drop_na = TRUE) %>%\n  mutate(index_explain = case_when(index %in% c(\"residual\", \"lbound\") ~ \"낮을수록 좋음\",\n                                    TRUE ~ \"높을수록 좋음\"),\n         index_factor = factor(index, levels = c(\"semcoh\", \"residual\", \"heldout\", \"lbound\"),\n                               labels = c(\"1. 의미론적 일관성\", \"2. 잔차\", \"3. Held-out 가능도\", \"4. 하한\")))"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#성능-그래프",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#성능-그래프",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "3.2. 성능 그래프",
    "text": "3.2. 성능 그래프\n그래프로 만드는 작업은 아래와 같다.\n\noutput_mergeSearchK %>%\n  ggplot(aes(K, value, color = index_factor)) +\n  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n  geom_point(size = 2.5, alpha = 1, show.legend = FALSE) +\n  # geom_vline(xintercept = 25) + # 최종 주제 개수\n  facet_wrap(~ index_factor, scales = \"free\") + scale_x_continuous(\n    breaks = seq(5, 50, 5), limits = c(5, 50)) +\n  scale_color_manual(values = brewer.pal(name=\"Set1\", n=4), guide=\"none\") +\n  labs(x = \"K (토픽 개수)\",\n       y = NULL,\n       title = \"주제 개수에 따른 모델 성능 비교\",\n       subtitle = \"적정 주제 개수를 25개로 결정\"\n       ) + \n  theme_bw() +\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank())"
  },
  {
    "objectID": "posts/2023-03-23-housing-consultation-analysis-3/index.html#최종-주제-개수-결정",
    "href": "posts/2023-03-23-housing-consultation-analysis-3/index.html#최종-주제-개수-결정",
    "title": "주거상담기록 데이터 분석 (3): 토픽모델링 주제 수 결정",
    "section": "3.3. 최종 주제 개수 결정",
    "text": "3.3. 최종 주제 개수 결정\n위 그래프를 보고 최종 주제 개수는 25개로 결정했다. 의미론적 일관성과 가능도는 클수록, 잔차와 하한은 낮을수록 좋은 점을 고려해 25개가 최적의 개수로 선정했다. 이를 표시하면 아래와 같다.\n\noutput_mergeSearchK %>%\n  ggplot(aes(K, value, color = index_factor)) +\n  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n  geom_point(size = 2.5, alpha = 1, show.legend = FALSE) +\n  geom_vline(xintercept = 25) + # 최종 주제 개수\n  facet_wrap(~ index_factor, scales = \"free\") + scale_x_continuous(\n    breaks = seq(5, 50, 5), limits = c(5, 50)) +\n  scale_color_manual(values = brewer.pal(name=\"Set1\", n=4), guide=\"none\") +\n  labs(x = \"K (토픽 개수)\",\n       y = NULL,\n       title = \"주제 개수에 따른 모델 성능 비교\",\n       subtitle = \"적정 주제 개수를 25개로 결정\"\n       ) + \n  theme_bw() +\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank())"
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "",
    "text": "토픽모델링 주제 이름을 짓는 과정을 다룬다. 주제 명명은 주제별 주요 단어와 대표문서를 참고한다."
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html#모델링-사전-준비",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html#모델링-사전-준비",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "1.1. 모델링 사전 준비",
    "text": "1.1. 모델링 사전 준비\n\n패키지 불러오기\n필요한 패키지를 불러온다.\n\n## 패키지 불러오기\npacman::p_load(\n  \"tidyverse\",  \"tidytext\", \"stm\", \"data.table\", \"knitr\", \"gt\")\n\n\n\n데이터 전처리\n토픽모델링 전 데이터 불러오기와 정제 과정을 거친다. 예제 코드 따라하기 위해선 아래 샘플 데이터를 다운받으면 된다.\n\n\n\n샘플 데이터\n설명\n\n\n\n\ndb_record_v1\n주거상담 메타데이터\n\n\ndb_record_v2.csv\n주거상담 형태소분석 결과\n\n\ndict_userDefined.txt\n사용자정의 단어 사전\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n단, 아래 샘플 데이터는 전체 중 100개를 무작위로 추출한 DB며, 이후 토픽모델링에 활용한 전체 데이터와 다르다.\n\n\n먼저 데이터를 불러온다.\n\n# 메타데이터 불러오기\ndb_recordMeta <- fread(\"Data/db_record_v1.csv\")\n\n# 형태소분석된 데이터 불러오기\ndb_record_1a <- fread(\n  \"data/db_record_v2.csv\",\n  select = c(\"id_f\", \"cons_text\", \"cons_text_posJoined_main\") # 필요한 열만 불러오기\n  )\n\n# 사용자정의 사전 불러오기\nlist_userDefined <- fread(\"data/dict/dict_userDefined.txt\")[substr(V2, 1, 1) %in% \"N\"]$V1\n\n희소단어 제거로 토픽모델링 전 정제를 해준다.\n\n# 희소 단어 제거를 위한 단어빈도 표\nword_count <- db_record_1a %>%\n  unnest_tokens(word, cons_text_posJoined_main, token = stringr::str_split, pattern = \" \") %>% # 띄어쓰기 기준\n  count(word, sort = TRUE) %>%\n  mutate(prop = cumsum(n) / sum(n))\n\n# 제거할 단어 리스트\nlist_wordRemoved <- setdiff(      # 차집합\n  setDT(word_count)[n <= 1]$word, # 빈도가 1개 이하인 단어 리스트\n  list_userDefined                # 사용자정의 단어  \n  )\n\n# 희소단어 제거\ndb_record_f <- db_record_1a %>% \n  unnest_tokens(word, cons_text_posJoined_main, token = stringr::str_split, pattern = \" \") %>%  # 띄어쓰기 기준\n  filter(!(word %in% list_wordRemoved)) %>%                                                     # 희소단어 제거\n  # 제거 후 붙이기\n  group_by(id_f)  %>%\n  summarize(cons_text_posJoined_main = toString(word)) %>% ungroup() %>%                        \n  mutate(cons_text_posJoined_main = str_remove_all(cons_text_posJoined_main, pattern = \"//,|,\")) %>%\n  inner_join(db_recordMeta, by = \"id_f\") %>%                                                    # 메타데이터와 붙이기\n  # 주요 품사가 한 개인 경우는 제외\n  filter(str_count(cons_text_posJoined_main, pattern = \" \") > 0)\n\n\n\n모델링 준비\n\n## 준비\nprocessed <- as_tibble(db_record_f) %>%\n  textProcessor(\n    documents = .$cons_text_posJoined_main, \n    metadata = .,\n    lowercase = FALSE,\n    removepunctuation = FALSE,\n    removenumbers = FALSE,\n    removestopwords = FALSE,\n    stem = FALSE,\n    wordLengths = c(1, Inf),\n    language = \"na\"\n    )\n\nout <- prepDocuments(\n  processed$documents,\n  processed$vocab,\n  processed$meta)\n\ndocs <- out$documents; vocab <- out$vocab; meta <- out$meta"
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html#토픽모델링-1",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html#토픽모델링-1",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "1.2. 토픽모델링",
    "text": "1.2. 토픽모델링\n앞서 결정한 주제 개수로 stm 토픽모델링을 수행한다. 주제 개수는 25개며, 투입하는 공변인prevalence은 상담센터와 상담연도, 상담사가 지정한 주요상담욕구다. 공변인은 주제 확률 모형에서 고려해야 할 변수를 말하며, 주거상담은 센터별, 연도별, 주요상담욕구별로 다를 것이라고 생각했다.\n\nSET_TOPIC_NUMBER <- 25                                          # 최종 주제 개수 지정\n\ntm_fiited <- stm(\n  documents = docs, \n  vocab = vocab,\n  data = meta,                                                   \n  prevalence = ~ cons_center + cons_date_year + cons_type_adj,  # 공변인\n  K = SET_TOPIC_NUMBER,                                         # 주제 개수\n  init.type = \"Spectral\",\n  max.em.its = 500                                              # 수렴까지 반복 횟수\n)\n\n모델링 한 결과 값은 rds 포맷으로 저장 가능하다.\n\ntm_fiited <- write_rds(\n  tm_fiited, # stm model\n  \"result/stmFinal_fit.rds\" # 저장 루트\n  )"
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html#모델-불러오기",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html#모델-불러오기",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "2.1. 모델 불러오기",
    "text": "2.1. 모델 불러오기\n정해진 최종 주제 개수로 모델링한 stm 모델을 불러오자. 여기에서 전체 데이터로 stm 모델링한 결과를 불러올 수 있다.\n\ntm_fiited <- readRDS(\"result/stmFinal_fit.rds\")"
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html#주요-단어",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html#주요-단어",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "2.2. 주요 단어",
    "text": "2.2. 주요 단어\n주제별 주요 단어를 추출한다. 추출할 단어 개수를 설정하고 labelTopics로 단어를 확인한다.\n\nwordNumber <- 5 # 보여질 단어 개수\noutput_wordPerTopic_1a <- \n  labelTopics(tm_fiited, n = wordNumber)\n\n주제 별로 4가지 종류에 따른 주요 단어를 볼 수 있다:\n\nProb: 특정 토픽에서 나타날 확률이 높은 단어\nFrex: 특정 토픽에는 빈도가 높지만 다른 토픽엔 그렇지 않은 단어(가중 평균 적용)\nLift: 특정 토픽에 고유한 단어 강조(다른 토픽 빈도 활용)\nScore: 특정 토픽에 고유한 단어(다른 토픽 로그 빈도 활용)\n\n\n\n[1] \"Topic 1 Top Words:\\n \\t Highest Prob: 오후, 나누다, 후원, 건강, 나눔 \\n \\t FREX: 나눔, 점심, 참석, 추석, 행사 \\n \\t Lift: 물티슈, 사단, 사랑나눔재단, 송편, 행사 \\n \\t Score: 나눔, 나누다, 후원, 마스크, 참석 \\nTopic 2 Top Words:\\n \\t Highest Prob: 지원, 임대료, 체납, 납부, 임차료 \\n \\t FREX: 임차료, 미납, 주거비, 납부, 고지서\\n   ...\"\n\n\n주제별 주요 단어를 다루기 편한 word 파일로 내보내는 작업은 아래와 같다.\n\nrbind(\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$prob), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Prob\"],\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$frex), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Frex\"],\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$lift), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Lift\"],\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$score), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Score\"])[\n        , .(`주제 번호` = topicnums,\n            ` ` = \" \",\n            `단어 가중치` = factor(type, levels = c(\"Prob\", \"Frex\", \"Lift\", \"Score\")), \n            `주요 단어` = word_list, topicnums = NULL)][order(`주제 번호`, `단어 가중치`)] %>%\n  gt(groupname_col = \"주제 번호\") %>%\n  tab_footnote(footnote = \"\n  Prob: 특정 토픽에서 나타날 확률이 높은 단어, \n  Frex: 특정 토픽에는 빈도가 높지만 다른 토픽엔 그렇지 않은 단어(가중 평균 적용),\n  Lift: 특정 토픽에 고유한 단어 강조(다른 토픽 빈도 활용), \n  Score: 특정 토픽에 고유한 단어(다른 토픽 로그 빈도 활용)\n  \") %>% \n  gtsave(\"result/주요단어_주제라벨링용.docx\")\n\n단어별 4가지 가중치에 대해 주요 단어를 word 파일로 볼 수 있다."
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html#대표문서",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html#대표문서",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "2.3. 대표문서",
    "text": "2.3. 대표문서\n주제를 대표하는 문서를 추출하는 방법은 아래와 같다. 각 문서는 모든 토픽마다 gamma를 가지고, 이는 그 문서에 해당 토픽 출현 확률이다. 예를 들어 문서 1에 주제 2의 gamma 값이 0.612라면, 문서 2의 단어 중 약 61.2%가 주제 2에 속한다고 해석할 수 있다. 이를 반대로하면, 대표문서를 추출하기 위해선 주제별로 gamma가 높음 문서를 추출하면 된다.\n\noutput_mainDocumPerTopic_1a <- \n  tidy(tm_fiited, matrix=\"gamma\", document_names=meta$id_f) %>%           # gamma 계산\n  mutate(topic=paste0(\"Topic \", topic), document=as.factor(document)) %>% # 'Topic' 문자열 추가\n  mutate(topic=fct_reorder(topic, parse_number(topic))) %>%               # Topic 순서 지정\n  left_join(meta, by = c(\"document\" = \"id_f\")) %>%                        # 메타데이터와 결합\n  distinct(topic, cons_text_posJoined_main, .keep_all = TRUE) %>%         # 주제별 상담 중복 제거\n  group_by(document) %>% slice_max(gamma, n = 1) %>% ungroup() %>%        # 각 문서별 gamma 최대값 주제만 남김\n  group_by(topic) %>% slice_max(gamma, n = 5)                             # 주제별로 5개 문서 제시\n\nas.data.table(output_mainDocumPerTopic_1a)[\n  , .(`주제 번호` = topic, \n      `문서 번호` = document, `상담기록` = cons_text, \n      `정제상담기록` = cons_text_posJoined_main, gamma)] %>%\n  gt() %>%\n  gtsave(\"result/대표문서_주제라벨링용.docx\")"
  },
  {
    "objectID": "posts/2023-03-24-housing-consultation-analysis-4/index.html#주제-명명",
    "href": "posts/2023-03-24-housing-consultation-analysis-4/index.html#주제-명명",
    "title": "주거상담기록 데이터 분석 (4): 토픽모델링 주제 명명",
    "section": "2.4. 주제 명명",
    "text": "2.4. 주제 명명\n주요 단어와 대표 문서를 보고 주제 이름을 연구자가 직접 지어주어야 한다.\n여기서 예시로 든 샘플이 아닌 원데이터를 쓴 경우, 주제 12의 주요 단어와 대표 문서는 다음과 같았다.\n\n\n\n\n\n\n\n\n\n가중치\n주요 단어 15개\n\n\n\n\nProb\n대출, 안내, 문의, 청년, 신청, 임차보증금, 소득, 가능, 은행, 서울시, 근로, 이자지원, 기준, 금액, 보험\n\n\nFrex\n이자지원, 임차보증금, 추천서, 전세자금, 대출, 버팀목전세대출, 은행, 융자, 청년전세임대, 서울주거포털, 청년, 하나은행, 득실, 청년월세지원, 증명\n\n\nLift\n한국주택금융공사, 근로계약서, 원천, 금리, 버팀목전세대출, 보금자리, 보람일자리, 서울주거포털, 이자지원, 임차보증금, 전세자금, 징수, 청년월세, 청년월세지원, 추천서\n\n\nScore\n대출, 청년, 임차보증금, 이자지원, 소득, 추천서, 은행, 버팀목전세대출, 전세자금, 증명, 근로, 문의, 신청, 보험, 발급\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n토픽 번호\n상담기록\ngamma\n\n\n\n\nTopic 12\n* 청년임차보증금대출 1. 근로청년으로 지원, 소득증명원에 대한 문의 - 19년 7월 이직하여 소득증명원을 입력하려고 하는데 7월 이후 현직장 근로소득 원천징수영수증을 첨부하라고 함. - 전년도소득증명원과 현직장 근로소득 원천징수영수증 두가지 모두 첨부해야 하는지와 근로소득 기간에 대해 문의함. ㅡ> 소득증명원에 대해서 더 구체적인 사항은 청년임차보증금 전월세팀(02-2133-7047)로 문의할 것을 안내함.\n0.9714335\n\n\nTopic 12\n* 서울시 청년 임차보증금 이자지원 사업 1. 연소득이 4천만원을 초과하면 아예 지원이 불가능한 것인지 문의함 ㅡ> 연소득 4천만원 이하인 것이 신청 자격이기때문에 초과를 한다면 신청 불가능한 것이라고 안내함 2. 이것 말고 이용할 수 있는 상품이 있는지 문의함 ㅡ> 청년전용 버팀목전세자금, 청년 맞춤형 전세자금 대출 안내함 3. 중복가능한 상품인지 문의함 ㅡ> 중복이 가능한 것은 청년임차보증금 이자지원과 청년 맞춤형 전세자금 대출이라고 안내함\n0.9698277\n\n\n\n\n\n주제 12은 주요 단어로 대출과 임차보증금, 이자지원, 추천서 등이 주로 나타났고 대표문서에서도 청년임차보증금 제도 문의가 드러났다. 따라서 주제 12의 주제 이름은 청년임차보증금 문의로 지었다."
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "",
    "text": "토픽모델링 후 주제의 주요 단어나 분포 등을 추출하고 시각화한다. stm 패키지의 결과물 추출하는 함수 사용 후, 표 형태는 gt 패키지로 편집이 편하게 워드 파일 형식으로, 시각화는 ggplot2, treemapify 등으로 이해를 돕는 그래프를 만든다."
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html#패키지-불러오기",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html#패키지-불러오기",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "1.1. 패키지 불러오기",
    "text": "1.1. 패키지 불러오기\n분석에 필요한 패키지를 불러온다.\n\n## 패키지 불러오기\npacman::p_load(\n  \"tidyverse\",  \"tidytext\", \"stm\", \n  \"data.table\", \"knitr\", \"gt\", \"scales\", \"treemapify\", \"RColorBrewer\")"
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html#stm-결과-불러오기",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html#stm-결과-불러오기",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "1.2. stm 결과 불러오기",
    "text": "1.2. stm 결과 불러오기\n\n주제정보 불러오기\n이전 글에서 확정한 주제 정보를 불러온다.\n\nlist_topicLabled <- fread(\"result/stm_labelling/주제정보.csv\")[\n  , `:=` (\n    topicLabel_f = factor(x = paste0(\"주제 \", topicNumber_f, \": \", topicName_f),\n                          levels = paste0(\"주제 \", topicNumber_f, \": \", topicName_f),\n                          labels = paste0(\"주제 \", topicNumber_f, \": \", topicName_f)),\n    topicGroup_f = factor(topicGroup_f, \n                          levels = c(\"주거정보\", \"직접지원\", \"복지정보\", \"사례·상담관리\", \"기타\"),\n                          labels = c(\"주거정보\", \"직접지원\", \"복지정보\", \"사례·상담관리\", \"기타\"))\n    )]\n\n각 열에 대한 설명은 다음과 같다:\n\ntopicNumber_model: stm 모델에서 생성한 주제 번호\ntopicNumber_f: 실제로 쓰이는 주제 번호\ntopicName_f: 주제 이름\ntopicLabel_f: 주제 번호와 이름\ntopicGroup_f: 주제 그룹\n\n\n\n\n\n\n\nNote\n\n\n\ntopicNumber_f로 새로운 주제 번호를 부여하는 이유는 토픽모델링 주제 번호는 사실상 무작위라 보기 좋게 정렬하기 위함이다. 예를 들어, stm 모델의 주제 번호 6과 11, 7, 25, 8이 ‘주거정보’ 그룹에 속하며 차례로 제시하고 싶다면 이를 주제 번호 1과 2, 3, 4, 5로 바꿔주고 정렬하면 된다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntopicNumber_model\ntopicNumber_f\ntopicName_f\ntopicGroup_f\ntopicLabel_f\n\n\n\n\n6\n1\n공공임대 공고 단순 정보\n주거정보\n주제 1: 공공임대 공고 단순 정보\n\n\n11\n2\n공공임대 공고 상세 정보\n주거정보\n주제 2: 공공임대 공고 상세 정보\n\n\n7\n3\n공공임대 자격기준 정보\n주거정보\n주제 3: 공공임대 자격기준 정보\n\n\n25\n4\n공공임대 당첨 안내·상담\n주거정보\n주제 4: 공공임대 당첨 안내·상담\n\n\n8\n5\n문자알림서비스 등록\n주거정보\n주제 5: 문자알림서비스 등록\n\n\n17\n6\n제출서류 안내\n주거정보\n주제 6: 제출서류 안내\n\n\n4\n7\n주택물색\n주거정보\n주제 7: 주택물색\n\n\n\n\n\n\nstm 모델 불러오기\n확정된 주제로 수행한 모델을 불러온다.\n\ntm_fiited <- readRDS(paste0(\"Result/stm_f/stmFinal_fit.rds\"))\n\n\n\n\n\n\n\nNote\n\n\n\n이 stm 모델은 전체 데이터로 수행한 것이다. 이전 글의 샘플 데이터만으로 수행한 모델 결과가 아니다."
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html#주요-단어",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html#주요-단어",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "2.1. 주요 단어",
    "text": "2.1. 주요 단어\n주제별 주요 단어를 추출한다. 단어 개수를 지정하고 labelTopics로 추출하면 다음과 같다.\n\nwordNumber <- 7 # 보여질 단어 개수\n\noutput_wordPerTopic_1a <- \n  labelTopics(tm_fiited, n = wordNumber)\n\n추출한 단어는 다음과 같다.\n\noutput_wordPerTopic_1a\n\n\nlist 형태인 이 결과물을 표로 만들어주고 저장한다.\n\n## 각 가중치별 단어 표 생성\noutput_wordPerTopic_1b <- \n  rbind(\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$prob), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Prob\"],\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$frex), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Frex\"],\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$lift), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Lift\"],\n    melt(data.table(topicnums = c(output_wordPerTopic_1a$topicnums), output_wordPerTopic_1a$score), 1)[\n      , .(word_list = toString(value)), .(topicnums)][, type := \"Score\"])[\n        list_topicLabled, on = c(\"topicnums\" = \"topicNumber_model\"), topicLabel_f := topicLabel_f][\n          , .(주제명 = topicLabel_f,\n              ` ` = \" \",\n              `단어 가중치` = factor(type, levels = c(\"Prob\", \"Frex\", \"Lift\", \"Score\")), \n              `주요 단어` = word_list, topicnums = NULL)][\n                order(주제명, `단어 가중치`)] %>%\n  gt(groupname_col = \"주제명\") %>%\n  tab_footnote(footnote = \n  \"Prob: 특정 토픽에서 나타날 확률이 높은 단어, \n  Frex: 특정 토픽에는 빈도가 높지만 다른 토픽엔 그렇지 않은 단어(가중 평균 적용),\n  Lift: 특정 토픽에 고유한 단어 강조(다른 토픽 빈도 활용)\n  Score: 특정 토픽에 고유한 단어(다른 토픽 로그 빈도 활용)\") %>%\n  gtsave(paste0(\"Result/stm_output/result_wordPerTopic.docx\"))\n\n저장한 파일을 열어보면 편집하기 편한 워드 파일로 정리된 것을 볼 수 있다."
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html#대표문서",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html#대표문서",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "2.2. 대표문서",
    "text": "2.2. 대표문서\n주제별 대표문서를 추출한다. 대표문서로 선정하는 문서는 코드로 추출하기 보다, 직접 선별한 문서로 제시하는 것이 낫다. 민감한 주제를 제거하기도 하고, 대표문서 선정의 기준인 gamma (문서-주제 확률)가 높더라도 사람이 보기에 대표하는 문서로 와닿지 않는 경우가 있기 때문이다. 위에서 받은 예시 파일로 아래 작업을 수행해보자.\n\nfread(\"result/stm_labelling/documPerTopic.csv\")[\n  list_topicLabled, on = c(\"topicnums\" = \"topicNumber_model\")][ # 주제 정보와 결합\n    , .(topicLabel_f, ` ` = \" \", 대표문서, round(gamma, 3))] %>%\n  gt(groupname_col = \"topicLabel_f\") %>%\n  gtsave(\"result/stm_output/result_documPerTopic.docx\")\n\n대표문서도 편집하기 편하게 워드 파일로 저장된 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html#주제-분포-정보",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html#주제-분포-정보",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "3.1. 주제 분포 정보",
    "text": "3.1. 주제 분포 정보\n주제별로 얼마나 출현하는 지나 대표문서의 수와 비율, gamma 값 등을 살펴보고 결과물로 저장한다.\n\n문서-주제 출현율 matrix\ntidy로 먼저 문서-주제 출현율(gamma)를 생성하고 주제 정보와 결합해준다.\n\ntd_gamma <- data.table(tidy(tm_fiited, matrix = \"gamma\"))[\n  list_topicLabled, on = c(\"topic\" = \"topicNumber_model\")]\n\nsetnames(td_gamma, \"document\", \"id_f\") # 문서의 ID 이름을 document에서 id_f로 변경\n\n\n\n주제 출현율\n전체 문서에서 주제가 나타나는 비율은 각 주제별로 gamma의 평균을 구하면 된다.\n\ntd_gamma_topicRatio <- td_gamma[\n  , .(prop_gamma = mean(gamma)), .(topicLabel_f)]\n\n주제 1은 전체 문서에서 약 7.65%, 주제 2는 3.89% 출현(분포)하고 있다.\n\n\n\n\n\ntopicLabel_f\nprop_gamma\n\n\n\n\n주제 1: 공공임대 공고 단순 정보\n0.0759411\n\n\n주제 2: 공공임대 공고 상세 정보\n0.0388192\n\n\n주제 3: 공공임대 자격기준 정보\n0.0199404\n\n\n주제 4: 공공임대 당첨 안내·상담\n0.0255691\n\n\n주제 5: 문자알림서비스 등록\n0.0590426\n\n\n\n\n\n\n\n대표문서 비율\n전체 문서 중 주제별 대표문서의 수와 비중을 구해본다.\n\ntd_gamma_documRatio = td_gamma[order(id_f, -gamma)][    # 문서별로 gamma가 높은 순으로 나열\n  , .SD[1], .(id_f)][                                   # 문서별로 가장 gamma가 높은 주제만 남김; 대표문서 추출\n    , .(n_docum = .N), topicLabel_f][                   # 주제별 대표문서 개수 계산\n      , prop_docum := n_docum / sum(n_docum)]           # 전체 문서 중 주제별 대표문서 비중 계산\n\n주제 9로 배정된 대표문서는 32,394건으로 310,675건의 전체 문서 중 10.42%를 차지한다.\n\n\n\n\n\ntopicLabel_f\nn_docum\nprop_docum\n\n\n\n\n주제 9: 청약정보 안내\n32394\n0.1042697\n\n\n주제 5: 문자알림서비스 등록\n18729\n0.0602849\n\n\n주제 6: 제출서류 안내\n13256\n0.0426684\n\n\n주제 2: 공공임대 공고 상세 정보\n11057\n0.0355902\n\n\n주제 4: 공공임대 당첨 안내·상담\n1289\n0.0041490\n\n\n\n\n\n\n\n대표문서 gamma\n대표문서로 지정된 주제의 gamma 평균 등을 살펴본다. 대표문서 주제 gamma는 대표문서에서 가장 출현율이 높은 주제의 확률을 말한다. 이를 살펴보면 어떤 주제가 대표문서를 지정할 때, 그 평균 값이 높은지 등을 통해 주제를 더 이해할 수 있다.\n\ntd_gamma_DocumGamma <- td_gamma[order(id_f, -gamma)][     # 문서별로 gamma가 높은 순으로 나열\n  , .SD[1], .(id_f)][                                     # 문서별로 가장 gamma가 높은 주제만 남김; 대표문서 추출\n    , .(mean_gamma_docum = mean(gamma),                   # 주제별로 gamma의 통계값 산출\n        min_gamma_docum = min(gamma), \n        max_gamma_docum = max(gamma), \n        sd_gamma_docum = sd(gamma)), .(topicLabel_f)]\n\n주제 2로 배정된 문서에서, 평균 gamma 값은 0.651, 최대와 최소 값은 0.10과 0.977이다. 다른 주제와 비교하자면, 주제 10으로 배정된 문서의 주제 10 gamma 값은 주제 2의 경우와 다르게 0.250다. 이는 주제 2가 유사한 공고문 문구 및 형식을 문자 등으로 발송하는 경우가 대다수라 그 주제 단어 출현율이 높게 나타난 것이며(0.651), 주제 10의 경우는 주제 10의 출현율이 높아 주제 10의 대표문서로 배정되었더라도, 그 문서 내 주제 10 외 다른 주제도 섞인 경우가 많아 gamma 평균 값이 비교적 낮은 것(0.250)이다.\n\n\n\n\n\n\n\n\n\n\n\n\ntopicLabel_f\nmean_gamma_docum\nmin_gamma_docum\nmax_gamma_docum\nsd_gamma_docum\n\n\n\n\n주제 2: 공공임대 공고 상세 정보\n0.6509847\n0.0984721\n0.9770741\n0.2668229\n\n\n주제 10: 임대차 갈등 조정\n0.2502232\n0.0914454\n0.6585711\n0.0784729\n\n\n주제 11: 비주택 주거상향사업 문의\n0.2772359\n0.0904835\n0.7864116\n0.0949464\n\n\n주제 12: 주거비 체납 상담·지원\n0.3533641\n0.0939063\n0.8721365\n0.1318650\n\n\n주제 13: 주거비 체납 안내\n0.4801685\n0.0938046\n0.9465828\n0.2312453\n\n\n주제 14: 1인가구주택관리서비스(간편집수리)\n0.4459523\n0.1047969\n0.9506402\n0.1839126\n\n\n\n\n\n\n\n주제 분포 통계\n위 주제 출현율과 대표문서 비율, 대표문서 gamma를 합치고 추출한다.\n\nReduce(merge, \n       list(td_gamma_topicRatio, td_gamma_documRatio, td_gamma_DocumGamma))[ # 3개 DB를 합치기\n         , .(주제명 = topicLabel_f,\n             `주제 출현율` = percent(prop_gamma, accuracy = 0.01),\n             `대표문서 수(비율)` = paste0(comma(n_docum), \" (\", prop_docum, \")\"),\n             `대표문서의 평균 gamma` = number(mean_gamma_docum, accuracy = 0.01),\n             `대표문서의 표준편차 gamma` = number(sd_gamma_docum, accuracy = 0.01),\n             `대표문서의 최소 gamma` = number(min_gamma_docum, accuracy = 0.01),\n             `대표문서의 최대 gamma` = number(max_gamma_docum, accuracy = 0.01))\n         ][order(주제명)] %>%\n  fwrite(\"result/stm_output/result_distri_topic.csv\", bom = T)\n\n아래 코드로 시각화한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주제명\n주제 출현율\n대표문서 수(비율)\n대표문서의 평균 gamma\n대표문서의 표준편차 gamma\n대표문서의 최소 gamma\n대표문서의 최대 gamma\n\n\n\n\n주제 1: 공공임대 공고 단순 정보\n7.59%\n28,511 (0.091771143477911)\n0.39\n0.19\n0.08\n0.89\n\n\n주제 2: 공공임대 공고 상세 정보\n3.88%\n11,057 (0.0355902470427295)\n0.65\n0.27\n0.10\n0.98\n\n\n주제 3: 공공임대 자격기준 정보\n1.99%\n4,146 (0.0133451355918564)\n0.54\n0.28\n0.10\n0.97"
  },
  {
    "objectID": "posts/2023-03-25-housing-consultation-analysis-5/index.html#대표문서-비율-시각화",
    "href": "posts/2023-03-25-housing-consultation-analysis-5/index.html#대표문서-비율-시각화",
    "title": "주거상담기록 데이터 분석 (5): 토픽모델링 주제 정보",
    "section": "3.2. 대표문서 비율 시각화",
    "text": "3.2. 대표문서 비율 시각화\n주제별 대표문서가 차지하는 비율을 treemapify 패키지를 활용해서 시각화한다. 이 그래프는 특정 값을 면적으로 하는 사각형 형태로 보여준다. 아래는 각 나라 GDP를 대륙에 따라 만든 것이다. \n먼저 주제별 대표문서 수와 비중을 구한다. 위와 다른 점은 그래프에 표기할 label이 추가됐다.\n\ntd_gamma_documRatio = td_gamma[order(id_f, -gamma)][    # 문서별로 gamma가 높은 순으로 나열\n  , .SD[1], .(id_f)][                                   # 문서별로 가장 gamma가 높은 주제만 남김; 대표문서 추출\n    , .(n_docum = .N), .(topicLabel_f, topicGroup_f)][  # 주제별 대표문서 개수 계산\n      , prop_docum := percent(n_docum / sum(n_docum),   # 전체 문서 중 주제별 대표문서 비중 계산\n                              accuracy = 0.1)][ \n      , labels := paste0(topicLabel_f,                  # 그래프에 표시할 label\n                         \" (\", prop_docum, \")\")] \n\n## 대분류별 비중\nlist_topicGroup_prop <- toString(td_gamma_documRatio[, .(n_docum = sum(n_docum)), topicGroup_f][\n  , prop_docum := percent(n_docum / sum(n_docum), accuracy = 0.1)][ \n    , labels := paste0(topicGroup_f, \" (\", prop_docum, \")\")][order(topicGroup_f)]$labels)\n\n\nggplot(td_gamma_documRatio,\n       aes(area = n_docum, fill = topicGroup_f, label=labels, subgroup=topicGroup_f)) +\n  geom_treemap(start = \"topleft\", aes(alpha = n_docum), show.legend = F) +\n  geom_treemap_subgroup_border(start = \"topleft\", colour=\"white\", show.legend = F) +\n  geom_treemap_text(\n    start = \"topleft\",\n    fontface = \"italic\",\n    colour = \"white\",\n    place = \"centre\",\n    grow = F, show.legend = F, reflow = T)  +\n  geom_treemap_subgroup_text(\n    start = \"topleft\", place = \"centre\",\n    grow = T, show.legend = F, fontface = \"italic\", \n    alpha = 0.5,\n    colour = \"grey20\",\n    min.size = 0) +\n  scale_alpha_continuous(range = c(0.1, 1)) +\n  scale_fill_manual(values = brewer.pal(name=\"Set1\", n = 5)) +\n  labs(\n    title = \"대분류, 중분류별 주제 대표문서 분포\",\n    subtitle = \"높은 비중일수록 불투명하며 좌측 상단 위치\",\n    caption = paste0(\"* 비중: \", list_topicGroup_prop))\n\n\n대분류별로 보면 주거정보가 전체 약 50%로 왼쪽에 가장 크게 빨간색으로 분포한다. 주거정보 중에서는 주제 9(청약정보 안내)가 전체 10.4%로 좌측 상단에 위치하며, 가장 불투명하다. 우측 하단으로 좀 더 투명해지며 주제 1과 주제 5, 주제 11 순으로 위치한다.\n아래 코드로 저장한다.\n\nggsave(\"result/stm_output/result_distri_topic.png\", dpi = 400, width = 12, height = 7)"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "",
    "text": "토픽모델링 주제와 메타데이터 사이의 공변량를 분석한다. 메타데이터는 텍스트 외 주제와 관련될 수 있는 데이터를 말한다. 내담자 성별이나 상담 방법(유선상담 등)은 특정 주제가 많이 등장할 수도 있다. 이런 공변량(covariate)를 분석하며, 이는 stm 토픽모델링의 장점 중 하나다."
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#패키지-불러오기",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#패키지-불러오기",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "1.1. 패키지 불러오기",
    "text": "1.1. 패키지 불러오기\n필요한 패키지를 불러온다.\n\n## 패키지 불러오기\npacman::p_load(\n  \"tidyverse\",  \"tidytext\", \"stm\", \"data.table\", \"knitr\", \"gt\", \"tidystm\")"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#주제정보-불러오기",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#주제정보-불러오기",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "1.2. 주제정보 불러오기",
    "text": "1.2. 주제정보 불러오기\n주제정보를 불러오고 정제한다.\n\nlist_topicLabled <- fread(\"result/stm_labelling/주제정보.csv\")[\n  , `:=` (\n    topicLabel_f = factor(x = paste0(\"주제 \", topicNumber_f, \": \", topicName_f),\n                          levels = paste0(\"주제 \", topicNumber_f, \": \", topicName_f),\n                          labels = paste0(\"주제 \", topicNumber_f, \": \", topicName_f)),\n    topicGroup_f = factor(topicGroup_f, \n                          levels = c(\"주거정보\", \"직접지원\", \"복지정보\", \"사례·상담관리\", \"기타\"),\n                          labels = c(\"주거정보\", \"직접지원\", \"복지정보\", \"사례·상담관리\", \"기타\"))\n    )]"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#stm-모델-불러오기",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#stm-모델-불러오기",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "1.2. stm 모델 불러오기",
    "text": "1.2. stm 모델 불러오기\n최종 stm 모델을 불러온다.\n\ntm_fiited <- readRDS(\"Result/stm_f/stmFinal_fit.rds\")"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#메타데이터-불러오기",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#메타데이터-불러오기",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "1.3. 메타데이터 불러오기",
    "text": "1.3. 메타데이터 불러오기\n공변량 분석에 필요한 메타데이터를 불러온다.\n\nmeta <- fread(\"data/db_meta.csv\")\n\n모든 메타데이터가 아닌 이번 실습에선 성별(user_gender)과 상담유형(cons_class; depth-심층상담/basic-초기상담), 상담방법(cons_way)이 주거상담 기록과 관련 있다고 가정하고 메타데이터로 준비했다.\n\n\n\n\n\n\nNote\n\n\n\n실제 연구 용역에선 위 성별, 상담유형, 상담방법 외 상담연도나 상담경로 등 다른 공변량 변수도 고려했다.\n\n\n\nmeta %>% slice(1:5) %>% kable()"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#메타데이터-전처리",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#메타데이터-전처리",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "1.4. 메타데이터 전처리",
    "text": "1.4. 메타데이터 전처리\n메타데이터를 모델에 넣기 전, 정제하는 과정이다. 특히, 남자와 여자, 심층상담과 초기상담 등 범주형 변수는 바꿔줘야 한다.\n\nmeta_aj <- meta[\n  , `:=` (user_gender = ifelse(user_gender %in% \"남자\", 0, 1), # 여성을 1, 남성을 0로 재정의\n          cons_class = ifelse(cons_class %in% \"depth\", 0, 1),  # 초기상담을 1, 심층상담을 0으로 재정의\n          cons_way = case_when(                                # 5개 상담방법 정의\n            cons_way %in% c(\"유선\", \"전화\") ~ \"유선상담\",\n            cons_way %in% c(\"내방\") ~ \"내방상담\", \n            cons_way %in% c(\"이동상담\", \"현장상담\") ~ \"이동상담\", \n            cons_way %in% c(\"방문\") ~ \"방문상담\", \n            TRUE ~ \"기타\"))] %>% as_tibble()"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#모델링-값-추출",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#모델링-값-추출",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "모델링 값 추출",
    "text": "모델링 값 추출\n성별과 주거상담 주제가 어떤 관계가 있는 지를 예시로 살펴보자. extract.estimateEffect은 모델링 결과를 확인하는 데 도움을 준다.\n\nextract.estimateEffect(\n  prep,                       # 공변량모델델\n  covariate = \"user_gender\",  # 보려는 공변량 \n  method = \"difference\",      # 남자-여자 차이\n  cov.value1 = \"남자\",        # 0으로 지정한 label(남성)\n  cov.value2 = \"여자\"         # 1으로 지정한 label(여성)\n) \n\n여기서는 모델에서 할당한 주제 1은 남자가 여자에 비해 더 많은 것(estimate = 0.00587 > 0)으로 나타났다.\n\nread_rds(\"result/t1.rds\") %>% slice(1:3) %>% kable()"
  },
  {
    "objectID": "posts/2023-03-26-housing-consultation-analysis-6/index.html#모델링-시각화",
    "href": "posts/2023-03-26-housing-consultation-analysis-6/index.html#모델링-시각화",
    "title": "주거상담기록 데이터 분석 (6): 주제 공변량 분석",
    "section": "모델링 시각화",
    "text": "모델링 시각화\n\n성별에 따른 주제 분석\n위 모델링 결과를 시각화하면 다음과 같다.\n\nextract.estimateEffect(\n  prep, \n  covariate = \"user_gender\", \n  #topics = 2, \n  method = \"difference\",\n  cov.value1 = \"남자\",\n  cov.value2 = \"여자\"\n)  %>% \n  left_join(list_topicLabled, by = c(\"topic\" = \"topicNumber_model\")) %>%\n  mutate(topicLabel_f = factor(topicLabel_f, levels = rev(list_topicLabled$topicLabel_f))) %>%\n  ggplot(aes(x = topicLabel_f, y = estimate, group = topicGroup_f)) +\n  geom_segment(aes(y = 0, \n                   x = topicLabel_f, \n                   yend = estimate, \n                   xend = topicLabel_f), \n               color = \"black\") +\n  # geom_errorbar(aes(ymin = ci.lower, ymax = ci.upper), position='dodge') +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0)+\n  coord_flip() +\n  facet_grid(topicGroup_f ~ ., scales = \"free\", space = \"free\",switch = \"y\") +\n  labs(\n    x = \"\", y = \"여성 대비 남성 내담자 주제 출현 계수\", \n    subtitle = \"오른쪽에 위치할수록 남성 내담자에서 더 등장하는 주제\",\n    title = \"내담자 성별에 따른 주제 분석\"\n  ) +\n  theme_bw() +\n  theme(strip.placement = \"outside\") \n\n\n위 결과를 해석하면, 남성 내담자에게 더 자주 등장하는 주제는 주제 16(이사비ˑ생활용품 지원)과 주제 11(비주택 주거상향사업 문의), 주제 7(주택물색)이다. 반면 여성 내담자는 주제 9(청약정보 안내)와 주제 5(문자알림서비스 등록) 등 주제가 자주 등장한다.\n\n\n상담방법에 따른 주제 분석\n보고서에 쓴 상담방법에 따른 주제 분석 그래프는 다음과 같다.\n\n상담방법이 기타인 경우, 주제 2 출현 확률이 높고 이는 주제 2(공공임대 공고 상세 정보) 주거상담을 문자로 보낼 때 기타로 주로 지정하는 것을 반영한다. 이동상담의 경우는 주제 9(청약정보 안내), 방문상담은 주제 14(1인가구주택관리서비스)가 자주 나타난다."
  },
  {
    "objectID": "posts/2023-04-19-sentiment-analysis/index.html",
    "href": "posts/2023-04-19-sentiment-analysis/index.html",
    "title": "한국어 감성 분석 비교: 네이버 vs 구글",
    "section": "",
    "text": "한국어 자연어 처리에서 감성 분석 서비스를 하는 구글과 네이버의 성능을 비교한다."
  },
  {
    "objectID": "posts/2023-04-19-sentiment-analysis/index.html#네이버-감성분석",
    "href": "posts/2023-04-19-sentiment-analysis/index.html#네이버-감성분석",
    "title": "한국어 감성 분석 비교: 네이버 vs 구글",
    "section": "2.1. 네이버 감성분석",
    "text": "2.1. 네이버 감성분석\n\n감성분석 준비\nCLOVA Sentiment 서비스 설명과 가이드라인은 여기에서 볼 수 있다. 콘솔 접속과 서비스 활성화, 인증정보 등을 따라하면 된다.\n\n\nR을 활용한 네이버 감성분석\n아래 코드로 할당받은 네이버 감성분석 API를 활용하여 샘플 데이터를 감성분석한다.\n\n# Define API endpoint and headers\nurl <- \"https://naveropenapi.apigw.ntruss.com/sentiment-analysis/v1/analyze\"\nclient_id = \"\"     # 할당된 클라이언트 ID\nclient_secret = \"\" # 할당된 API key\n\nheaders_user <- c(\n  \"X-NCP-APIGW-API-KEY-ID\" = client_id,\n  \"X-NCP-APIGW-API-KEY\" = client_secret,\n  \"Content-Type\" = \"application/json\"\n)\n\ndb_sent_1c <- db_sent_1b %>%\n  group_split(id)\n\n\ndb_sent_2a <- rbindlist(\n  future_map(db_sent_1c, function(x){\n    \n    sent_content_input <- x$sent_content\n    \n    seti_analysis_respon <- POST(\n      url = \"https://naveropenapi.apigw.ntruss.com/sentiment-analysis/v1/analyze\",\n      body = toJSON(list(content = sent_content_input) , auto_unbox = TRUE, pretty = TRUE), \n      add_headers(.headers = headers_user),\n      encode = \"json\") %>% \n      content()\n    \n    doc_dt <- data.table(\n      doc_content = sent_content_input,\n      doc_senti = seti_analysis_respon$document$sentiment,\n      doc_confi_neg = seti_analysis_respon$document$confidence$negative,\n      doc_confi_posi = seti_analysis_respon$document$confidence$positive,\n      doc_confi_neut = seti_analysis_respon$document$confidence$neutral\n    )\n    \n    # create a data.table of sentence-level information\n    sent_dt <- data.table(\n      sent_content = sapply(seti_analysis_respon$sentences, function(x) x$content),\n      sent_offset = sapply(seti_analysis_respon$sentences, function(x) x$offset),\n      sent_length = sapply(seti_analysis_respon$sentences, function(x) x$length),\n      sent_sentiment = sapply(seti_analysis_respon$sentences, function(x) x$sentiment),\n      sent_confi_neg = sapply(seti_analysis_respon$sentences, function(x) x$confidence$negative),\n      sent_confi_posi = sapply(seti_analysis_respon$sentences, function(x) x$confidence$positive),\n      sent_confi_neut = sapply(seti_analysis_respon$sentences, function(x) x$confidence$neutral),\n      sent_highlight_offset = sapply(seti_analysis_respon$sentences, function(x) x$highlights[[1]]$offset),\n      sent_highlight_length = sapply(seti_analysis_respon$sentences, function(x) x$highlights[[1]]$length)\n    )\n    \n    print(\"end\")\n    \n    comb_dt <- cbind(id = x$id, doc_dt, sent_dt)\n    \n  }))\n\n감성분석 결과는 다음과 같다. 여기서 doc와 senti로 시작하면 각각 문서와 그 문서를 구성하는 문장으로 나타낸다. 네이버 클로버 감성분석은 분석하는 문서(doc)의 문장을 자동으로 분류해준다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ndoc_content\ndoc_senti\ndoc_confi_neg\ndoc_confi_posi\ndoc_confi_neut\nsent_content\nsent_offset\nsent_length\nsent_sentiment\nsent_confi_neg\nsent_confi_posi\nsent_confi_neut\nsent_highlight_offset\nsent_highlight_length\n\n\n\n\n1\n그늘도 많고 쉴자리도 많아요.\npositive\n0.0002367\n99.99958\n0.0001859\n그늘도 많고 쉴자리도 많아요.\n0\n16\npositive\n0.0000142\n0.9999746\n0.0000112\n0\n15\n\n\n2\n공기좋음\npositive\n0.0367223\n99.95812\n0.0051644\n공기좋음\n0\n4\npositive\n0.0022033\n0.9974868\n0.0003099\n0\n4\n\n\n3\n맑은하늘을 맘껏볼수있는곳\npositive\n0.0001375\n99.99956\n0.0003061\n맑은하늘을 맘껏볼수있는곳\n0\n13\npositive\n0.0000083\n0.9999734\n0.0000184\n0\n13"
  },
  {
    "objectID": "posts/2023-04-19-sentiment-analysis/index.html#구글-감성분석",
    "href": "posts/2023-04-19-sentiment-analysis/index.html#구글-감성분석",
    "title": "한국어 감성 분석 비교: 네이버 vs 구글",
    "section": "2.2. 구글 감성분석",
    "text": "2.2. 구글 감성분석\n구글 감성 분석은 여기에서 자세히 배울 수 있다. 네이버 감성분석과 같이 R를 활용하여 구글 감성 분석 API를 쓰는 방법은 추후에 업데이트하고자 한다."
  },
  {
    "objectID": "posts/2023-04-19-sentiment-analysis/index.html#분류-정확도-요약",
    "href": "posts/2023-04-19-sentiment-analysis/index.html#분류-정확도-요약",
    "title": "한국어 감성 분석 비교: 네이버 vs 구글",
    "section": "3.1. 분류 정확도 요약",
    "text": "3.1. 분류 정확도 요약\n네이버 감성분석은 F1 점수가 0.84로 구글 감성분석 0.642보다 더 높았다.\n\n\n\n\n\n\n\n\n\n\n\n지표\n네이버 감성분석\n구글 감성분석\n구글 감성분석(조정 후)\n\n\n\n\nF1-score\n0.840\n0.642\n0.718\n\n\nBalanced Accuracy\n0.879\n0.736\n0.788"
  },
  {
    "objectID": "posts/2023-04-19-sentiment-analysis/index.html#비교-데이터셋-만들기",
    "href": "posts/2023-04-19-sentiment-analysis/index.html#비교-데이터셋-만들기",
    "title": "한국어 감성 분석 비교: 네이버 vs 구글",
    "section": "3.2. 비교 데이터셋 만들기",
    "text": "3.2. 비교 데이터셋 만들기\n분석할 감성분석 결과 DB를 만들어준다.\n\ndb_sent_2b <- unique(db_sent_2a, by = \"id\") %>%\n  select(id, doc_senti)\n\ndb_sent_f <- db_sent_1b %>%\n  left_join(db_sent_2b, by = \"id\") %>%\n  mutate(\n    senti_label_google = case_when(\n      between(senti_score_google, 0.25, 1) ~ \"긍정\",\n      between(senti_score_google, -0.25, 0.25) ~ \"중립\",\n      TRUE ~ \"부정\"),\n    senti_label_naver = case_when(\n      doc_senti == \"positive\" ~ \"긍정\",\n      doc_senti == \"neutral\" ~ \"중립\",\n      TRUE ~ \"부정\")\n    ) %>%\n  select(id, sent_content, senti_label_google, senti_label_naver, label) %>%\n    mutate_at(c(\"senti_label_google\", \"senti_label_naver\", \"label\"), factor, levels = c(\"부정\", \"중립\", \"긍정\"))\n\n\nconf_mat_google <- confusion_matrix(\n  targets = db_sent_f$label,\n  predictions = db_sent_f$senti_label_google\n  )\n\nconf_mat_naver <- confusion_matrix(\n  targets = db_sent_f$label,\n  predictions = db_sent_f$senti_label_naver\n  )"
  },
  {
    "objectID": "posts/2023-04-19-sentiment-analysis/index.html#감성분석-상세",
    "href": "posts/2023-04-19-sentiment-analysis/index.html#감성분석-상세",
    "title": "한국어 감성 분석 비교: 네이버 vs 구글",
    "section": "3.3. 감성분석 상세",
    "text": "3.3. 감성분석 상세\n\n네이버 감성분석\n\nplot_confusion_matrix(conf_mat_naver$`Confusion Matrix`[[1]], \n                      class_order = c(\"긍정\", \"중립\", \"부정\"),\n                      add_sums = TRUE)  +\n  ggplot2::labs(title = \"네이버 감성분석 분류 정확도 결과\")\n\nWarning in plot_confusion_matrix(conf_mat_naver$`Confusion Matrix`[[1]], :\n'ggimage' is missing. Will not plot arrows and zero-shading.\n\n\nWarning in plot_confusion_matrix(conf_mat_naver$`Confusion Matrix`[[1]], :\n'rsvg' is missing. Will not plot arrows and zero-shading.\n\n\nWarning in plot_confusion_matrix(conf_mat_naver$`Confusion Matrix`[[1]], :\n'ggnewscale' is missing. Will not use palette for sum tiles.\n\n\n\n\n\n\n\n잘못 분류한 문장\n\n중립 문장을 긍정으로 잘못 분류한 경우\n\n\n\n\n\n\n\n\ndb_sent_2b <- unique(db_sent_2a, by = \"id\") %>%\n  select(id, doc_senti, doc_confi_posi, doc_confi_neut, doc_confi_neg)\n\ndb_sent_f <- db_sent_1b %>%\n  left_join(db_sent_2b, by = \"id\") %>%\n  mutate(\n    senti_label_google = case_when(\n      between(senti_score_google, 0.25, 1) ~ \"긍정\",\n      between(senti_score_google, -0.25, 0.25) ~ \"중립\",\n      TRUE ~ \"부정\"),\n    senti_label_naver = case_when(\n      doc_senti == \"positive\" ~ \"긍정\",\n      doc_senti == \"neutral\" ~ \"중립\",\n      TRUE ~ \"부정\")\n    )\n\ndata.table(db_sent_f)[label == \"부정\" & senti_label_naver == \"중립\"][\n  , .(id, sent_content, label, senti_label_naver, doc_confi_posi, doc_confi_neut, doc_confi_neg)]\n\n     id                       sent_content label senti_label_naver\n 1: 210       힐링하러 갔다가 헬링 당한 곳  부정              중립\n 2: 216             타는 장소도 너무 멀고,  부정              중립\n 3: 217       인파도 많고 난잡하기도 하다.  부정              중립\n 4: 221       입장료가 비싼편이라 생각해요  부정              중립\n 5: 226          살짝 실망스러운 모습이라.  부정              중립\n 6: 230                  넘 사람들이 많다.  부정              중립\n 7: 231                인성개씨발동네 ㄹㅇ  부정              중립\n 8: 236           노잼이고 막 볼것도 없다.  부정              중립\n 9: 246              불볕더위에 땡볓이지만  부정              중립\n10: 251                   주차장이 없어요.  부정              중립\n11: 260          대중교통이 차라리 나아요.  부정              중립\n12: 269            걍 집에서 잠이나 자세요  부정              중립\n13: 271         지금 시즌은 별로인 듯해요.  부정              중립\n14: 274                     주차장 비좁음.  부정              중립\n15: 275                      밤엔 넘 춥다.  부정              중립\n16: 277                     사람 너무많다.  부정              중립\n17: 279    기대가 커서 그런가 실망도 크다.  부정              중립\n18: 280 볼것도없고그냥뭐동네뒷산갔다온느낌  부정              중립\n19: 302                      너무 사람많음  부정              중립\n20: 310                주차 완전 힘듭니다.  부정              중립\n21: 315          주차가 가장 짜증 났습니다  부정              중립\n22: 316                  개선이 필요합니다  부정              중립\n23: 325                        극악의 주차  부정              중립\n24: 335                차도 너무 막혔네요.  부정              중립\n25: 340                주차 진짜 쒯입니다.  부정              중립\n26: 341      맞기라도 하면.상상도 싫으네요  부정              중립\n27: 343         저녁 때 사람이 많긴 하지만  부정              중립\n28: 358           시설이 노후됫습니다 ㅠㅜ  부정              중립\n29: 360              리모델링이 시급하네요  부정              중립\n30: 363   ㅜㅠ 바람도 춥고 해서 아쉬웠네요  부정              중립\n31: 364         집회때문에 너무 불편했어요  부정              중립\n32: 365   광장에 사람들이 넘 많이 모였어요  부정              중립\n33: 372                       쓰레기냄새남  부정              중립\n34: 373                  의자도 별로 없고,  부정              중립\n35: 381                사람구경만 졸라하다  부정              중립\n36: 391                    찾는 이가 없음.  부정              중립\n37: 396                       주차공간협소  부정              중립\n38: 398                            주차 헬  부정              중립\n     id                       sent_content label senti_label_naver\n    doc_confi_posi doc_confi_neut doc_confi_neg\n 1:    0.002516054       99.99630   0.001187555\n 2:    0.076974176       97.06975   2.853277200\n 3:    0.064776120       99.74305   0.192171300\n 4:    0.008725748       99.98264   0.008629889\n 5:    0.100952960       99.67683   0.222217780\n 6:    0.271925180       98.22211   1.505968600\n 7:    0.123892054       93.11234   6.763764400\n 8:    0.023809950       99.92928   0.046910760\n 9:    0.066535760       99.89360   0.039866920\n10:    0.006073157       99.67338   0.320546840\n11:    0.062801205       99.88726   0.049936870\n12:    0.392281380       99.26622   0.341496530\n13:    0.023040438       99.92218   0.054779444\n14:    0.789424060       61.47467  37.735905000\n15:    0.002088725       99.99641   0.001495643\n16:    0.145121800       99.69920   0.155682190\n17:    0.055910440       99.67062   0.273466380\n18:    0.309557470       99.57802   0.112426504\n19:    0.017323079       99.94631   0.036366973\n20:    0.601401900       97.76577   1.632826100\n21:    0.789424060       61.47467  37.735905000\n22:    0.496102060       99.48928   0.014619959\n23:    0.193368870       99.60049   0.206140460\n24:    0.022184083       99.87048   0.107330120\n25:    0.021959830       99.90194   0.076099320\n26:    0.004861866       99.98780   0.007340889\n27:    0.017429445       96.66999   3.312579900\n28:    0.091496530       99.81611   0.092393720\n29:    0.070926390       99.47028   0.458789140\n30:    0.351080270       98.24281   1.406106100\n31:    0.026297357       99.95812   0.015587593\n32:    0.430049840       93.90658   5.663370600\n33:    0.068197310       99.88186   0.049946714\n34:    0.265411100       99.56754   0.167052850\n35:    0.660176630       92.43296   6.906865600\n36:    0.182308810       99.71406   0.103631936\n37:    0.036754236       99.89362   0.069629155\n38:    0.248254840       99.72199   0.029763816\n    doc_confi_posi doc_confi_neut doc_confi_neg\n\n\n\n\n\n구글 감성분석\n\nplot_confusion_matrix(conf_mat_google$`Confusion Matrix`[[1]], \n                      class_order = c(\"긍정\", \"중립\", \"부정\"),\n                      add_sums = TRUE)\n\nWarning in plot_confusion_matrix(conf_mat_google$`Confusion Matrix`[[1]], :\n'ggimage' is missing. Will not plot arrows and zero-shading.\n\n\nWarning in plot_confusion_matrix(conf_mat_google$`Confusion Matrix`[[1]], :\n'rsvg' is missing. Will not plot arrows and zero-shading.\n\n\nWarning in plot_confusion_matrix(conf_mat_google$`Confusion Matrix`[[1]], :\n'ggnewscale' is missing. Will not use palette for sum tiles.\n\n\n\n\n\n\n구글 감성분석 parameter\n문장별로 감성점수를 제공하는 구글 자연어처리 서비스에서 점수를 긍정과 부정, 중립으로 나누는 기준이 모호하다는 생각이 들었다. 아래 예시와 같이, ‘싸늘하다’ 등은 부정적으로 느껴지지만 구글 감성점수 기준에서는 중립에 속한다.\n\n긍정과 긍정을 결정하는 기준을 다르게해서 구글 감성분석 라벨을 다시 매겨보았다.\n\nlist_parameter <- expand.grid(\n  threshold_posi = seq(0.25, 0.95, 0.05),\n  threshold_neg = seq(0.25, -0.95, -0.05)) %>%\n  mutate(id = row_number()) %>%\n  group_split(id)\n\nresult_senti_google_param <- rbindlist(future_map(\n  list_parameter, function(x){\n    \n    \n    db_sent_f <- db_sent_1b %>%\n      left_join(db_sent_2b, by = \"id\") %>%\n      mutate(\n        senti_label_google = case_when(\n          between(senti_score_google, x$threshold_posi, 1) ~ \"긍정\",\n          between(senti_score_google, -1, x$threshold_neg) ~ \"부정\",\n          TRUE ~ \"중립\")) %>%\n      select(id, sent_content, senti_label_google, label) %>%\n      mutate_at(c(\"senti_label_google\", \"label\"), factor, levels = c(\"부정\", \"중립\", \"긍정\"))\n    \n    conf_mat_google <- confusion_matrix(\n      targets = db_sent_f$label,\n      predictions = db_sent_f$senti_label_google)\n    \n    out <- cbind(x, F1 = conf_mat_google$F1, `Balanced Accuracy` = conf_mat_google$`Balanced Accuracy`)\n  }\n  \n))\n\n내가 매긴 라벨이 정답이라는 가정 하에, 기준 값을 조정하면 F1 점수는 0.7181로 기준 0.5534에서 높아진다.\n\n\n\n\n\nid\nF1\nthreshold_posi\nthreshold_neg\n\n\n\n\n127\n0.7181\n0.55\n-0.15\n\n\n128\n0.7181\n0.60\n-0.15\n\n\n142\n0.7181\n0.55\n-0.20\n\n\n143\n0.7181\n0.60\n-0.20\n\n\n129\n0.7174\n0.65\n-0.15\n\n\n\n\n\n\n\n\n잘못분류한 문장들"
  }
]